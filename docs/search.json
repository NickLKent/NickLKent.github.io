[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Resume",
    "section": "",
    "text": "Macalester College | St. Paul, MN\nMajor: Data Science\nMinor: Computer Science and Economics\nSeptember 2022 - Present"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Resume",
    "section": "",
    "text": "Macalester College | St. Paul, MN\nMajor: Data Science\nMinor: Computer Science and Economics\nSeptember 2022 - Present"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Resume",
    "section": "Experience",
    "text": "Experience\n\nUnitedHealth Group | Eden Prairie, MN\nData Analyst Intern\nMay 2024 - August 2024\nMacalester College | St. Paul, MN\nTeaching Assistant for COMP/STAT 112, STAT 155, ECON 119\nSeptember 2023 - Present"
  },
  {
    "objectID": "about.html#extracurriculars",
    "href": "about.html#extracurriculars",
    "title": "Resume",
    "section": "Extracurricular(s)",
    "text": "Extracurricular(s)\n\nMacalester Men’s Soccer\n\nMIAC All-Conference Honors(Fall 2022, Fall 2023, Fall 2024)\nUnited Soccer Coaches All-Region Honors(Fall 2023)\nCSC Academic All-District Honors(Fall 2023)\nMIAC Academic All-Conference(Fall 2023)\nRising Scot Scholar Athlete of the Year(2023-2024)\n\nMacalester College MSCS Student Advisory Board\n\nMembers of this board serve as a bridge of communication between students and staff in the Math, Statistics, and Computer Science departments.\nAdvisory Board members also introduce and facilitate their own inclusivity and accessibility initiatives.\n\nMacalester College Data Science Club\n\nFounder of the data science club where students and faculty come together weekly to practice and share data science skills.\nAt our meetings we will work on items ranging from mini projects, skill exercises, personal portfolios, etc."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "Resume",
    "section": "Skills",
    "text": "Skills\n\nComputer skills :  R/RStudio, Java, Javascript, HTML, CSS, SQL, Python, React, Excel"
  },
  {
    "objectID": "projects/SP1/SP_1_Trade:Disasters.html",
    "href": "projects/SP1/SP_1_Trade:Disasters.html",
    "title": "Modeling Trade VS Natural Disasters",
    "section": "",
    "text": "Loading the data\n\n# Necessary packages\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(GGally)\nlibrary(broom) \n\n# Loading in the dataset\nnatural_disaster &lt;- read_csv(\"nat_disaster_data.csv\")\ncountry_trade_data &lt;- read_csv(\"country_trade_data.csv\")\n\n\n\nData wrangling/cleaning\n\n# Creating a data set for each countries number of natural disasters per year over the last 23 years. \nnat_dis_per_year_per_country &lt;- natural_disaster %&gt;% \n  group_by(Country, `Start Year`) %&gt;% \n  summarise(num_of_nat_dis = n()) %&gt;% \n  mutate(\"Start Year\" = as.character(`Start Year`))\n\n# Obtain a data set for each countries trade percentage of GDP over the last 23 years. \ntrade_per_year_per_country &lt;- country_trade_data %&gt;% \n  pivot_longer(cols = c(\"1999\",\"2000\",\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\"), names_to = \"Year\", values_to = \"Trade share of GDP\") %&gt;% \n  select(`Country Name`, Year, `Trade share of GDP`)\n\n# We now have a data set that contains a line for each country and year combination, and then along with that is the countries trade share of GDP and number of natural disasters in that year. \nmodel_data &lt;- trade_per_year_per_country %&gt;% \n  left_join(nat_dis_per_year_per_country, by = c(\"Country Name\" = \"Country\", \"Year\" = \"Start Year\")) %&gt;% \n  filter(!is.na(num_of_nat_dis))\n\n\n\nCreating a model\n\nmodel &lt;- model_data %&gt;%\n  with(lm(`Trade share of GDP` ~ num_of_nat_dis))\n\nmodel %&gt;% \n  tidy()\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic  p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)       81.8      1.11       73.9 0       \n2 num_of_nat_dis    -2.09     0.187     -11.2 3.41e-28\n\nmodel %&gt;% \n  tidy() %&gt;% \n  summarize(lb = estimate - 2*std.error, ub = estimate + 2*std.error)\n\n# A tibble: 2 × 2\n     lb    ub\n  &lt;dbl&gt; &lt;dbl&gt;\n1 79.6  84.0 \n2 -2.47 -1.72\n\n\n\n\nTakeaways from the model\n\n\nConfidence interval for the slope does not contain zero and all the values in the interval are negative which means we can be sure that there is a negative relationship between natural disasters and the trade share of GDP.\nThe intercept tells us that if a country faces no natural disasters then on average their trade share of GDP will be about 81.77 %.\nThe estimate for the slope tells us that on average for every one additional natural disaster that occurs in a country their trade share of GDP will drop by an average of 2.092 %.\nP-value for both the intercept and the slope are extremely low meaning that these estimates are statistically significant."
  },
  {
    "objectID": "projects/SP3/Lib_Ed.html",
    "href": "projects/SP3/Lib_Ed.html",
    "title": "Liberal Arts Education at Macalester",
    "section": "",
    "text": "How do students choose to take classes in a breadth of areas to fulfill both general requirements and major specific ones? The data set we are examining shows data from Macalester students who graduated from 2013-2018 regarding information like graduation year, sex, major(s), minor(s), number of classes taken in departments/divisions etc. Because the data provides a holistic view on a student’s education, we want to examine how students of any major interact with classes. How does the number of majors change the number of classes in each division? How does major affect the number of unique departments taken? How does the division of majors affect the number of classes taken within each division? We hope that by approaching the topic from a few different perspectives such as major count, unique departments, and department variety, we can learn a little bit about what it means to be a student, regardless of what category one may fall into.\n\n\n\nThis visualization shows the difference in the types of classes that students take depending on how many majors they have. Selecting one, two, or three majors will subsect the data and show how many classes in each division students with the selected number of majors took. Interestingly enough, there is little difference between the number of classes a student takes with one or two majors. But you can see that when you look at the students with three majors, the Natural Science and Mathematics bar is clearly taller than the rest of the divisions. The reason for this may be because it’s easier to have multiple majors within the Natural Science and Math division because there is some overlap between the requirements for majors within that division. When looking at students with one major, Natural Science and Math, Humanities, and Social Sciences are relatively equal which shows that majors in those divisions are equally popular among students with one major. Examining the graph that shows that of students with two majors, the Humanities majors take slightly more classes than the Natural Science and Math majors. The reason for this may be because those majors often pair well with majors in other divisions. For example, students may choose to major in a language and pair that with a major in the Interdisciplinary division. There also might not be enough data to draw clear conclusions; out of the approximately 2,500 students looked at, there are about 1800 students who had one major and 650 who had two. This number drops significantly when looking at students who had three majors (19).\n \n\n\n\nHow do students take courses in and out of their major? Do some majors attract students that take many courses outside of their major? One way to explore how “diverse” Mac students are in terms of their course selection is by looking at how many total departments there are in which a student took a course. This visualization shows how many unique departments each major has taken, with the significant caveat that this filters out all students with more than one major. Doing so results in a different amount of students in each department, but is worth doing because including students with multiple majors would skew certain majors to further extremes. For example, as seen by the visualization of “Number of classes in a division by major” we see that science and math majors are more likely to have multiple majors, and those majors are often within the same division. If we included double and triple majors in this data set, science and math majors could appear significantly less diverse because those students with multiple majors are taking many classes in few departments. Between 2013 and 2018, the average number of unique department prefixes a student took over their time at Macalester was about 12 (11.88 precisely). Some notable departments with low amounts of “department diversity” are Russian Studies, Music, and Women, Gender and Sexuality Studies, and some notable departments with high amounts are American Studies, Sociology, and Latin American Studies. While Interdisciplinary and Social Sciences are the two divisions with the highest average diversity, and Humanities and Natural Science and Math are the two with the least, the divisions are still relatively similar to each other. In fact, all the divisions have between 12-13 majors, with the exception of Fine Arts, which only has three majors. One would expect, based on stereotypes, that STEM majors, for example, might be the least flexible. \nThis visualization, however, shows that while there is definitely a discrepancy between majors, it is not especially large; the range between Russian Studies, the minimum, and Latin American Studies, the maximum is only 4.25. If we separate based on division, we get a similar picture, and find that the average number of departments taken for Humanities is the same as Math, around 12.1!  The most important conclusion of this data is that Macalester does a good job of encouraging students to take a wide variety of classes. All divisions have a similar range of unique departments (about three), and when the visualization is expanded to show all majors, the range grows to only 4.25.\n\n\n\nOur last visualization shows the average number of classes taken in each division based on the division of the major. It’s important to note that students who had more than one major were counted twice; for example, a Math and Biology double major would have been considered two “people” and course count was considered twice. This is fine to do because if you are majoring in biology and math your class distribution should be shown as a biology major and a math major separately, since that reflects your course choice/interests as a student.\nThe first thing we notice is that students with majors within the Natural Science and Mathematics division tend to be the most focused on their own division; the discrepancy between Science/Math classes they take and classes they take in other divisions is the greatest. On average, the number of classes that a Natural Science and Math division major would take in that division is 20. On the other hand, the same students would take on average 16 classes outside of the Natural Science and Math division. This makes sense because students majoring in Natural Science and Math departments will often have to take extra classes outside of their majors department. So for example, a physics major would have to take many physics classes, but then also take some math classes and possibly other science classes. All of these classes would be counted as a Natural Science and Math class, therefore the count of Natural Science and Math classes for a physics major(and other math and science majors) would be unusually high.\n  The division that had the most equal spread of classes among the five division were the Interdisciplinary majors, which makes sense because Interdisciplinary inherently means relating to more than one branch of knowledge. Those that are learning about multiple subjects would essentially have to take classes in multiple divisions to fulfill the requirements. Another division of majors that had a more balanced variety in the average number of classes taken among divisions is the Social Science majors. To visualize the statements a little more, on average someone majoring in an Interdisciplinary major will take about 12 classes in the Interdisciplinary division, and take about 23 classes outside of the Interdisciplinary division. Similarly, students majoring in Social Sciences will on average take about 14 classes in the Social Science division, and take 20 classes outside of the Social Sciences division. So students in these divisions tend to take more classes outside of their majors division than classes in the division of their major, and you don’t see a huge change in the number of classes taken in their majors division, and classes taken outside of their majors division.\n \nFrom this visualization we learned that the division of your major directly affects the number of classes that you take in each of the five divisions.\n\n\n\nTo answer the question of what the Macalester Liberal Arts experience looks like, we used a data set describing information about Macalester Students from 2013-2018, and created three different interactive visualizations that explore different aspects of student course choice. Our first visualization explored the relationship between number of majors and classes in each division. In it, we see that single-major students tend to take more classes in the Natural Science and Mathematics, Non-divisional and Social Science divisions, revealing which are the most popular. Students with 3 majors tend to take more classes in Natural Science and Math, showing that people with three majors are more likely majoring in a math or science department. In our second visualization, showing the relationships between major and amount of unique departments taken, we learn that Macalester does a good job of encouraging a variety of class choices, and regardless of major, students take classes in between 10-14 departments, with the exact average being 11.88. Our final visualization shows the relationship between the number of classes taken in each division based on major. This visualization shows that students in the Natural Science and Math division tend to stay within their division more than other divisions. Interdisciplinary, Social Science or Humanities have a more even spread between classes inside and outside of their division.\n\n\n\nData Wrangling\nShiny App Code"
  },
  {
    "objectID": "projects/SP3/Lib_Ed.html#what-does-it-look-like-to-be-a-liberal-arts-student-at-macalester",
    "href": "projects/SP3/Lib_Ed.html#what-does-it-look-like-to-be-a-liberal-arts-student-at-macalester",
    "title": "Liberal Arts Education at Macalester",
    "section": "",
    "text": "How do students choose to take classes in a breadth of areas to fulfill both general requirements and major specific ones? The data set we are examining shows data from Macalester students who graduated from 2013-2018 regarding information like graduation year, sex, major(s), minor(s), number of classes taken in departments/divisions etc. Because the data provides a holistic view on a student’s education, we want to examine how students of any major interact with classes. How does the number of majors change the number of classes in each division? How does major affect the number of unique departments taken? How does the division of majors affect the number of classes taken within each division? We hope that by approaching the topic from a few different perspectives such as major count, unique departments, and department variety, we can learn a little bit about what it means to be a student, regardless of what category one may fall into.\n\n\n\nThis visualization shows the difference in the types of classes that students take depending on how many majors they have. Selecting one, two, or three majors will subsect the data and show how many classes in each division students with the selected number of majors took. Interestingly enough, there is little difference between the number of classes a student takes with one or two majors. But you can see that when you look at the students with three majors, the Natural Science and Mathematics bar is clearly taller than the rest of the divisions. The reason for this may be because it’s easier to have multiple majors within the Natural Science and Math division because there is some overlap between the requirements for majors within that division. When looking at students with one major, Natural Science and Math, Humanities, and Social Sciences are relatively equal which shows that majors in those divisions are equally popular among students with one major. Examining the graph that shows that of students with two majors, the Humanities majors take slightly more classes than the Natural Science and Math majors. The reason for this may be because those majors often pair well with majors in other divisions. For example, students may choose to major in a language and pair that with a major in the Interdisciplinary division. There also might not be enough data to draw clear conclusions; out of the approximately 2,500 students looked at, there are about 1800 students who had one major and 650 who had two. This number drops significantly when looking at students who had three majors (19).\n \n\n\n\nHow do students take courses in and out of their major? Do some majors attract students that take many courses outside of their major? One way to explore how “diverse” Mac students are in terms of their course selection is by looking at how many total departments there are in which a student took a course. This visualization shows how many unique departments each major has taken, with the significant caveat that this filters out all students with more than one major. Doing so results in a different amount of students in each department, but is worth doing because including students with multiple majors would skew certain majors to further extremes. For example, as seen by the visualization of “Number of classes in a division by major” we see that science and math majors are more likely to have multiple majors, and those majors are often within the same division. If we included double and triple majors in this data set, science and math majors could appear significantly less diverse because those students with multiple majors are taking many classes in few departments. Between 2013 and 2018, the average number of unique department prefixes a student took over their time at Macalester was about 12 (11.88 precisely). Some notable departments with low amounts of “department diversity” are Russian Studies, Music, and Women, Gender and Sexuality Studies, and some notable departments with high amounts are American Studies, Sociology, and Latin American Studies. While Interdisciplinary and Social Sciences are the two divisions with the highest average diversity, and Humanities and Natural Science and Math are the two with the least, the divisions are still relatively similar to each other. In fact, all the divisions have between 12-13 majors, with the exception of Fine Arts, which only has three majors. One would expect, based on stereotypes, that STEM majors, for example, might be the least flexible. \nThis visualization, however, shows that while there is definitely a discrepancy between majors, it is not especially large; the range between Russian Studies, the minimum, and Latin American Studies, the maximum is only 4.25. If we separate based on division, we get a similar picture, and find that the average number of departments taken for Humanities is the same as Math, around 12.1!  The most important conclusion of this data is that Macalester does a good job of encouraging students to take a wide variety of classes. All divisions have a similar range of unique departments (about three), and when the visualization is expanded to show all majors, the range grows to only 4.25.\n\n\n\nOur last visualization shows the average number of classes taken in each division based on the division of the major. It’s important to note that students who had more than one major were counted twice; for example, a Math and Biology double major would have been considered two “people” and course count was considered twice. This is fine to do because if you are majoring in biology and math your class distribution should be shown as a biology major and a math major separately, since that reflects your course choice/interests as a student.\nThe first thing we notice is that students with majors within the Natural Science and Mathematics division tend to be the most focused on their own division; the discrepancy between Science/Math classes they take and classes they take in other divisions is the greatest. On average, the number of classes that a Natural Science and Math division major would take in that division is 20. On the other hand, the same students would take on average 16 classes outside of the Natural Science and Math division. This makes sense because students majoring in Natural Science and Math departments will often have to take extra classes outside of their majors department. So for example, a physics major would have to take many physics classes, but then also take some math classes and possibly other science classes. All of these classes would be counted as a Natural Science and Math class, therefore the count of Natural Science and Math classes for a physics major(and other math and science majors) would be unusually high.\n  The division that had the most equal spread of classes among the five division were the Interdisciplinary majors, which makes sense because Interdisciplinary inherently means relating to more than one branch of knowledge. Those that are learning about multiple subjects would essentially have to take classes in multiple divisions to fulfill the requirements. Another division of majors that had a more balanced variety in the average number of classes taken among divisions is the Social Science majors. To visualize the statements a little more, on average someone majoring in an Interdisciplinary major will take about 12 classes in the Interdisciplinary division, and take about 23 classes outside of the Interdisciplinary division. Similarly, students majoring in Social Sciences will on average take about 14 classes in the Social Science division, and take 20 classes outside of the Social Sciences division. So students in these divisions tend to take more classes outside of their majors division than classes in the division of their major, and you don’t see a huge change in the number of classes taken in their majors division, and classes taken outside of their majors division.\n \nFrom this visualization we learned that the division of your major directly affects the number of classes that you take in each of the five divisions.\n\n\n\nTo answer the question of what the Macalester Liberal Arts experience looks like, we used a data set describing information about Macalester Students from 2013-2018, and created three different interactive visualizations that explore different aspects of student course choice. Our first visualization explored the relationship between number of majors and classes in each division. In it, we see that single-major students tend to take more classes in the Natural Science and Mathematics, Non-divisional and Social Science divisions, revealing which are the most popular. Students with 3 majors tend to take more classes in Natural Science and Math, showing that people with three majors are more likely majoring in a math or science department. In our second visualization, showing the relationships between major and amount of unique departments taken, we learn that Macalester does a good job of encouraging a variety of class choices, and regardless of major, students take classes in between 10-14 departments, with the exact average being 11.88. Our final visualization shows the relationship between the number of classes taken in each division based on major. This visualization shows that students in the Natural Science and Math division tend to stay within their division more than other divisions. Interdisciplinary, Social Science or Humanities have a more even spread between classes inside and outside of their division.\n\n\n\nData Wrangling\nShiny App Code"
  },
  {
    "objectID": "projects/SP4/FinalReport.html",
    "href": "projects/SP4/FinalReport.html",
    "title": "Final Report",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(broom)\n\n\n\n\nmenu_stat &lt;- read_csv(\"menu_items.csv\")\n\n\n\n\n\n#This cleans the data so that it gets rid of all of the restaurants and menu items that we will not be using.  \nfinal_menu_stat &lt;- menu_stat %&gt;% \n  rename(category = Food_Category) %&gt;% \n  select(restaurant, Item_Name, category, Calories, Total_Fat, Carbohydrates, Protein, Sugar) %&gt;% \n  filter(!category == \"Toppings & Ingredients\") %&gt;% \n  filter(!is.na(Calories)) %&gt;% \n  filter(restaurant %in% c(\"McDonald's\", \"Burger King\", \"Wendy's\")) %&gt;% \n  filter(category %in% c(\"Burgers\", \"Fried Potatoes\"))"
  },
  {
    "objectID": "projects/SP4/FinalReport.html#load-data",
    "href": "projects/SP4/FinalReport.html#load-data",
    "title": "Final Report",
    "section": "",
    "text": "menu_stat &lt;- read_csv(\"menu_items.csv\")"
  },
  {
    "objectID": "projects/SP4/FinalReport.html#data-cleaning",
    "href": "projects/SP4/FinalReport.html#data-cleaning",
    "title": "Final Report",
    "section": "",
    "text": "#This cleans the data so that it gets rid of all of the restaurants and menu items that we will not be using.  \nfinal_menu_stat &lt;- menu_stat %&gt;% \n  rename(category = Food_Category) %&gt;% \n  select(restaurant, Item_Name, category, Calories, Total_Fat, Carbohydrates, Protein, Sugar) %&gt;% \n  filter(!category == \"Toppings & Ingredients\") %&gt;% \n  filter(!is.na(Calories)) %&gt;% \n  filter(restaurant %in% c(\"McDonald's\", \"Burger King\", \"Wendy's\")) %&gt;% \n  filter(category %in% c(\"Burgers\", \"Fried Potatoes\"))"
  },
  {
    "objectID": "projects/SP4/FinalReport.html#numerical-summaries",
    "href": "projects/SP4/FinalReport.html#numerical-summaries",
    "title": "Final Report",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nfinal_menu_stat %&gt;% \n  group_by(category, restaurant) %&gt;% \n  summarise(n(), median(Calories))\n\n# A tibble: 6 × 4\n# Groups:   category [2]\n  category       restaurant  `n()` `median(Calories)`\n  &lt;chr&gt;          &lt;chr&gt;       &lt;int&gt;              &lt;dbl&gt;\n1 Burgers        Burger King    55                500\n2 Burgers        McDonald's     21                530\n3 Burgers        Wendy's        21                390\n4 Fried Potatoes Burger King    16                350\n5 Fried Potatoes McDonald's     10                230\n6 Fried Potatoes Wendy's        14                420"
  },
  {
    "objectID": "projects/SP4/FinalReport.html#visualizations",
    "href": "projects/SP4/FinalReport.html#visualizations",
    "title": "Final Report",
    "section": "Visualizations",
    "text": "Visualizations\n\nbluetheme &lt;- theme(\n  plot.title = element_text(family = \"Times New Roman\", face = \"bold\", size = (15), colour = \"steelblue4\"),\n  legend.title = element_text(colour = \"steelblue\", face = \"bold.italic\", family = \"Helvetica\"),\n  legend.text = element_text(face = \"italic\", colour = \"steelblue4\", family = \"Helvetica\", ),\n  axis.title = element_text(family = \"Times New Roman\", size = (12), colour = \"steelblue4\", face = \"bold\"),\n  axis.text = element_text(family = \"Times New Roman\", colour = \"cornflowerblue\", size = (10))\n)\n\nfinal_menu_stat %&gt;% \n  filter(category == \"Burgers\") %&gt;% \n  arrange(desc(Calories)) %&gt;% \n  ggplot(aes(x = reorder(restaurant, Calories), y = Calories)) +\n  geom_boxplot(color = \"darkblue\", fill = \"lightgray\") +\n  labs(x = \"Restaurant\", y = \"Calories\", title = \"Calorie Content of Burger's at Different Fast Food Restaurants\") +\n  bluetheme\n\n\n\n\n\n\n\nfinal_menu_stat %&gt;% \n  filter(category == \"Fried Potatoes\") %&gt;% \n  arrange(desc(Calories)) %&gt;% \n  ggplot(aes(x = reorder(restaurant, Calories), y = Calories)) +\n  geom_boxplot(color = \"darkblue\", fill = \"lightgray\") +\n  labs(x = \"Restaurant\", y = \"Calories\", title = \"Calorie Content of Fried Potatoes at Different Fast Food Restaurants\", color = \"darkblue\") +\n  bluetheme"
  },
  {
    "objectID": "projects/SP4/FinalReport.html#fit-model",
    "href": "projects/SP4/FinalReport.html#fit-model",
    "title": "Final Report",
    "section": "Fit Model",
    "text": "Fit Model\n\n# Model with burger as the intercept\nfinal_model &lt;- final_menu_stat %&gt;% \n  with(lm(Calories ~ category*restaurant))\n\n#Model with fried potatoes as the intercept\nfinal_model_opposite &lt;- final_menu_stat %&gt;% \n  mutate(category = relevel(droplevels(factor(category)), ref = \"Fried Potatoes\")) %&gt;% \n  with(lm(Calories ~ category*restaurant))\n\n\nRegression Model\n\\[E[ Calories |  category * restaurant] = \\beta_0 + \\beta_1categoryFriedPotatoes + \\beta_2RestaurantMcDonald's + \\beta_3RestaurantWendy's \\] \\[+ \\beta_4categoryFriedPotatoes:RestaurantMcDonald's + \\beta_5categoryFriedPotatoes:RestaurantWendy's \\]"
  },
  {
    "objectID": "projects/SP4/FinalReport.html#inference",
    "href": "projects/SP4/FinalReport.html#inference",
    "title": "Final Report",
    "section": "Inference",
    "text": "Inference\n\nCoefficient Estimates\n\ntidy(final_model)\n\n# A tibble: 6 × 5\n  term                                     estimate std.error statistic  p.value\n  &lt;chr&gt;                                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                                 551.       34.6    15.9   1.95e-32\n2 categoryFried Potatoes                     -166.       72.9    -2.28  2.42e- 2\n3 restaurantMcDonald's                         27.8      65.8     0.422 6.74e- 1\n4 restaurantWendy's                           -14.6      65.8    -0.222 8.25e- 1\n5 categoryFried Potatoes:restaurantMcDona…   -145.      123.     -1.18  2.40e- 1\n6 categoryFried Potatoes:restaurantWendy's     26.7     115.      0.233 8.16e- 1\n\ntidy(final_model_opposite)\n\n# A tibble: 6 × 5\n  term                                 estimate std.error statistic      p.value\n  &lt;chr&gt;                                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)                             385.       64.2     6.00  0.0000000182\n2 categoryBurgers                         166.       72.9     2.28  0.0242      \n3 restaurantMcDonald's                   -117.      103.     -1.13  0.260       \n4 restaurantWendy's                        12.1      93.9     0.129 0.897       \n5 categoryBurgers:restaurantMcDonald's    145.      123.      1.18  0.240       \n6 categoryBurgers:restaurantWendy's       -26.7     115.     -0.233 0.816       \n\n\n\n\nConfidence Intervals\n\nfinal_model %&gt;% \n  confint()\n\n                                                2.5 %    97.5 %\n(Intercept)                                  482.8012 619.74424\ncategoryFried Potatoes                      -310.5105 -22.03492\nrestaurantMcDonald's                        -102.4839 158.03370\nrestaurantWendy's                           -144.8649 115.65274\ncategoryFried Potatoes:restaurantMcDonald's -387.4052  97.85543\ncategoryFried Potatoes:restaurantWendy's    -200.1917 253.68958\n\nfinal_model_opposite %&gt;% \n  confint()\n\n                                          2.5 %    97.5 %\n(Intercept)                           258.05041 511.94959\ncategoryBurgers                        22.03492 310.51054\nrestaurantMcDonald's                 -321.70006  87.70006\nrestaurantWendy's                    -173.69231 197.97803\ncategoryBurgers:restaurantMcDonald's  -97.85543 387.40521\ncategoryBurgers:restaurantWendy's    -253.68958 200.19175"
  },
  {
    "objectID": "projects/SP4/Final_Phase2.html",
    "href": "projects/SP4/Final_Phase2.html",
    "title": "Final_Phase2",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggmap)\nlibrary(broom)\n\n\nResearch Question\n\nHow does calorie content differ between McDonalds, Wendy’s, and Burger King, comparing the same food category (i.e. Sandwiches, Salads, Burgers)? In other words, how does fast food restaurant effect the relationship between food category and calorie content of a fast food menu item?\n\n\nmenu_stat &lt;- read_csv(\"menu_items.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n\n\nData Cleaning\n\n#This cleans the data so that it gets rid of all of the menus items that have NA's for nutritional facts. \nmenu_stat2 &lt;- menu_stat %&gt;% \n  rename(category = Food_Category) %&gt;% \n  select(restaurant, Item_Name, category, Calories, Total_Fat, Carbohydrates, Protein, Sugar) %&gt;% \n  filter(!category == \"Toppings & Ingredients\") %&gt;% \n  filter(!is.na(Calories), !is.na(Total_Fat), !is.na(Carbohydrates), !is.na(Protein)) \n\n\n\nLooking at the data\n\n#Second data set idea\nfinal_menu_stat2 &lt;- menu_stat2 %&gt;% \n  filter(restaurant %in% c(\"McDonald's\", \"Burger King\", \"Wendy's\")) %&gt;% \n  filter(category %in% c(\"Burgers\", \"Fried Potatoes\"))\n\n#Burgers are the most common fast food item in America and people often get fries with those burgers. \n\n\n\nRegression Model\n\\[ E[ Calories |  category * restaurant] = \\beta_0 + \\beta_1categoryFriedPotatoes + \\beta_2RestaurantMcDonald's + \\beta_3RestaurantWendy's \\\\\n+ \\beta_4categoryFriedPotatoes:RestaurantMcDonald's + \\beta_8categoryFriedPotatoes:RestaurantWendy's \\]\n\n\nFitting the linear regression model\n\nWe chose to use a linear regression model with an interaction between the category of the food and the restaurant to predict calorie content. We chose this model because we are trying to examine how calorie content differs among different food categories at fast food restaurants, and we believe that food category might effect calorie content differently based on the restaurant that the food item comes from, and for that reason we chose a linear regression model with an interaction.\n\n\nOutcome variable = Calories(Quantitative)\n\n\nPredictor of interest = Food Category(Categorical)\n\n\nPrecision variables: fat, protein, carbohydrates\n\n\nEffect modifier: restaurant\n\n\nExplanatory variables that we will use in our model: the interaction between category and restaurant.\n\n\nfinal_model &lt;- final_menu_stat2 %&gt;% \n  with(lm(Calories ~ category*restaurant))\n\n# Next we want to know how do find the difference in burger king fries to mcdonalds fries? WE NEED TO REORDER THE VARIABLES?\nfinal_menu_stat2 %&gt;% \n  mutate(category = relevel(droplevels(factor(category)), ref = \"Fried Potatoes\")) %&gt;% \n  with(lm(Calories ~ category*restaurant))\n\n\nCall:\nlm(formula = Calories ~ category * restaurant)\n\nCoefficients:\n                         (Intercept)                       categoryBurgers  \n                              385.00                                166.27  \n                restaurantMcDonald's                     restaurantWendy's  \n                             -117.00                                 12.14  \ncategoryBurgers:restaurantMcDonald's     categoryBurgers:restaurantWendy's  \n                              144.77                                -26.75  \n\nfinal_menu_stat2 %&gt;% \n  filter(restaurant == \"Wendy's\", category == \"Fried Potatoes\") %&gt;% \n  summarise(mean(Calories))\n\n# A tibble: 1 × 1\n  `mean(Calories)`\n             &lt;dbl&gt;\n1             397.\n\nfinal_menu_stat2 %&gt;% \n  filter(restaurant == \"Wendy's\", category == \"Burgers\") %&gt;% \n  summarise(mean(Calories))\n\n# A tibble: 1 × 1\n  `mean(Calories)`\n             &lt;dbl&gt;\n1             537.\n\n\n\n\nCoefficients and more from the model\n\ntidy(lm(Calories ~ category*restaurant, data = final_menu_stat2))\n\n# A tibble: 6 × 5\n  term                                     estimate std.error statistic  p.value\n  &lt;chr&gt;                                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                                 551.       34.6    15.9   1.95e-32\n2 categoryFried Potatoes                     -166.       72.9    -2.28  2.42e- 2\n3 restaurantMcDonald's                         27.8      65.8     0.422 6.74e- 1\n4 restaurantWendy's                           -14.6      65.8    -0.222 8.25e- 1\n5 categoryFried Potatoes:restaurantMcDona…   -145.      123.     -1.18  2.40e- 1\n6 categoryFried Potatoes:restaurantWendy's     26.7     115.      0.233 8.16e- 1\n\n\n\n\nCreating Confidence Intervals\n\n# Code for confidence intervals\nfinal_model %&gt;% \n  confint()\n\n                                                2.5 %    97.5 %\n(Intercept)                                  482.8012 619.74424\ncategoryFried Potatoes                      -310.5105 -22.03492\nrestaurantMcDonald's                        -102.4839 158.03370\nrestaurantWendy's                           -144.8649 115.65274\ncategoryFried Potatoes:restaurantMcDonald's -387.4052  97.85543\ncategoryFried Potatoes:restaurantWendy's    -200.1917 253.68958\n\n\n\n\nRelevant Hypothesis test’s\n\nHo: restaurantMcDonald’s = 0 –&gt; There is no difference in the expected calories of a McDonald’s Burger to a Burger King burger.\n\n\nHa: restaurantMcDonald’s != 0 –&gt; There is a difference in the expected calories of a McDonald’s Burger to a Burger King burger.\n\n\nHo: restaurantWendy’s = 0 –&gt; There is no difference in the expected calories of a Wendy’s Burger to a Burger King burger.\n\n\nHa: restaurantWendy’s != 0 –&gt; There is a difference in the expected calories of a Wendy’s Burger to a Burger King burger.\n\n\n\nWhat do we learn from this model?\n\nWhen we look at this model the first thing you notice is the intercept coefficient. This coefficient estimates that the calorie content of a Burger King burger is on average 551.27 calories. The you can see from the ‘restaurantMcDonald’s’ coefficient that the estimated calorie increase in a McDonald’s burger from a Burger King burger is 27.7 calories. Additionally the interval from -102.48 to 158.03 gives us a range of plausible values for the true difference in calorie content of a McDonald’s burger and a Burger King burger, and we are 95% confident that the true population value for this coefficient will fall in this interval. Next, you can see from the ‘restaurantWendy’s’ coefficient that the estimated calorie decrease in a Wendy’s burger from a Burger King burger is 14.6 calories. Furthermore, the interval from -102.48 to 158.03 gives us a range of possible values for the true difference in calorie content of Wendy’s burger and a Burger King burger, and we are 95% confident that the true population parameter will fall in that interval. In this model you can see the estimated difference in the calorie content when comparing a burger to fried potatoes among the three restaurant. Overall, from this model we learn that McDonald’s burger tend to be the “least healthy” in terms of calorie content, followed by Burger King and then Wendy’s. We could also find the same thing out about fried potatoes once we learn to reorder the variables and get an intercept for fries instead of burgers.\n\n\n\nHypothesis test decisions\n\n(restaurantMcDonald’s: p-value = 0.6738501) This p-value means that there is about a 67 percent chance of seeing this large of an estimated difference between the calorie content of a McDonald’s burger and the calorie content of a Burger King burger if the null hypothesis is true. With a p-value of 0.67385019(for the ’restaurantMcDonald’s coefficient) there is not statistically significant evidence to suggest that there is a difference between the calorie content of a McDonald’s burger and a Burger King burger. Also, the confidence interval of -102.4839 to 158.03370 does contain the null value of 0. Because of both of these things, we once again fail to reject the null and cannot conclude that there is a difference in the expected calories of a McDonald’s Burger to a Burger King burger.\n\n\n(restaurantWendy’s: p-value = 0.8247979)This p-value means that there is about a 82 percent chance of seeing this large of an estimated difference between the calorie content of a Wendy’s burger and the calorie content of a Burger King burger if the null value is true. With a p-value of 0.8247979 there is not statistically significant evidence to suggest that there is a difference between the calorie content of a Wendy’s burger and a Burger King burger, and we fail to reject the null hypothesis. Also, the confidence interval of -144.8649 to 115.65274 does contain the null value of 0. Because of both of these things, we once again fail to reject the null and cannot conclude that there is a difference in the expected calories of a Wendy’s Burger to a Burger King burger.\n\n\n\nLimitations\n\nDuring our research and R code, we began to realize that there would be a couple limitations that will take place in our study. For instance, we were forced to exclude some fast food restaurants in our survey. The main reason we did this is because our data set contained over 96 different fast food restaurants, and if we included every single one of those(while using a regression model that examined the effect that restaurant has on calorie content) that would have made our linear model quite long and hard to work with. Another reason we chose to only work with McDonald’s, Burger King and Wendy’s is because we wanted to have nationwide chains (sorry Culvers) that were extremely popular and have locations all over the country. Additionally, we had to filter out a decent amount of menu items because we only chose to work with menu items that fell under the categories of salads, burgers, sandwiches, and fried Potatoes. The four categories that we chose for our model equation were items that were consistent among the restaurants that we chose to work with, they are looked at as some of the more common fast food menu items, they are all relatively equal in volume so the differences in calories among them wont strictly be due to the size of the foods, and lastly they are all considered entrees. For instance, we know that beverages have calories but it is very unfair to compare the calorie content of a drink to the calorie content of a burger or sandwhich."
  },
  {
    "objectID": "projects/SP5/ContentSummary_YesAssoc.html",
    "href": "projects/SP5/ContentSummary_YesAssoc.html",
    "title": "Content Summary #1",
    "section": "",
    "text": "This is the exact same experiment as the first one, except now the IQ score depends on one of the SNP’s.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(broom)\nlibrary(qqman)\n\n\nFor example usage please run: vignette('qqman')\n\nCitation appreciated but not required:\nTurner, (2018). qqman: an R package for visualizing GWAS results using Q-Q and manhattan plots. Journal of Open Source Software, 3(25), 731, https://doi.org/10.21105/joss.00731.\n\n\n\n\n100 observations 200,000 SNP’s\n\n\n\nIn each position(SNP) an observation either gets a AG, GA, AA, GG, each A/G represents what they got from each parent in that position.\n\n\n\nset.seed(42)\n\n# Number of people, SNPs per chromosome, and chromosomes\nnum_people &lt;- 100\nnum_snps_per_chr &lt;- 10000\nnum_chromosomes &lt;- 23\n\nsnp_grid &lt;- expand_grid(\n  Chromosome = 1:num_chromosomes,\n  Position = 1:num_snps_per_chr\n) %&gt;%\n  mutate(\n    SNP_ID = row_number(),\n    prob_A = runif(n(), 0, 1)\n  ) %&gt;%\n  mutate(prob_G = 1 - prob_A) \n\nhead(snp_grid)\n\n# A tibble: 6 × 5\n  Chromosome Position SNP_ID prob_A prob_G\n       &lt;int&gt;    &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1          1        1      1  0.915 0.0852\n2          1        2      2  0.937 0.0629\n3          1        3      3  0.286 0.714 \n4          1        4      4  0.830 0.170 \n5          1        5      5  0.642 0.358 \n6          1        6      6  0.519 0.481 \n\n\n\n\n\n\n# Create person data with SNPs and varying probabilities\npeople_data &lt;- tibble(Person_ID = 1:num_people) %&gt;%\n  crossing(snp_grid) %&gt;%\n  mutate(\n    Allele_1 = ifelse(runif(n()) &lt; prob_A, \"A\", \"G\"),\n    Allele_2 = ifelse(runif(n()) &lt; prob_A, \"A\", \"G\")\n  )\n\nhead(people_data)\n\n# A tibble: 6 × 8\n  Person_ID Chromosome Position SNP_ID prob_A prob_G Allele_1 Allele_2\n      &lt;int&gt;      &lt;int&gt;    &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n1         1          1        1      1  0.915 0.0852 A        A       \n2         1          1        2      2  0.937 0.0629 A        G       \n3         1          1        3      3  0.286 0.714  A        G       \n4         1          1        4      4  0.830 0.170  G        A       \n5         1          1        5      5  0.642 0.358  A        G       \n6         1          1        6      6  0.519 0.481  A        G       \n\n\n\n\n\n\n# Identify the minor allele for each SNP\nminor_alleles &lt;- people_data %&gt;%\n  group_by(SNP_ID) %&gt;%\n  mutate(\n    count_A = sum(Allele_1 == \"A\" | Allele_2 == \"A\"), \n    count_G = sum(Allele_1 == \"G\" | Allele_2 == \"G\")\n  ) %&gt;%\n  mutate(\n    Minor_Allele = if_else(count_A &lt; count_G, \"A\", \"G\"), \n    MAF = pmin(count_A, count_G) / (2 * num_people)\n  ) %&gt;%\n  select(SNP_ID, Minor_Allele, MAF) %&gt;%\n  distinct()\n\n# NOTE: This is part of quality control\nminor_alleles &lt;- minor_alleles %&gt;% \n  filter(MAF != 0)\n\nhead(minor_alleles)\n\n# A tibble: 6 × 3\n# Groups:   SNP_ID [6]\n  SNP_ID Minor_Allele   MAF\n   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1      1 G            0.075\n2      2 G            0.09 \n3      3 A            0.235\n4      4 G            0.19 \n5      5 G            0.3  \n6      6 A            0.35 \n\n\n\n\n\n\n# Counting the minor alleles at each SNP position\nminor_allele_counts &lt;- people_data %&gt;%\n  left_join(minor_alleles, by = \"SNP_ID\") %&gt;%\n  mutate(\n    Minor_Allele_Count = \n      (Minor_Allele == Allele_1) + (Minor_Allele == Allele_2)  # Vectorized count\n  ) %&gt;%\n  select(Person_ID, SNP_ID, Minor_Allele_Count) %&gt;%  # Keep only necessary columns\n  pivot_wider(names_from = SNP_ID, values_from = Minor_Allele_Count)\n\nhead(minor_allele_counts)\n\n# A tibble: 6 × 230,001\n  Person_ID   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1         1     0     1     1     1     1     1     0     1     2     1     2\n2         2     0     0     1     2     1     2     1     1     1     1     1\n3         3     0     0     0     1     1     0     1     0     1     1     2\n4         4     0     0     0     1     0     1     1     1     1     0     0\n5         5     0     0     0     0     1     0     2     0     1     1     1\n6         6     0     0     0     0     1     2     1     0     1     1     1\n# ℹ 229,989 more variables: `12` &lt;int&gt;, `13` &lt;int&gt;, `14` &lt;int&gt;, `15` &lt;int&gt;,\n#   `16` &lt;int&gt;, `17` &lt;int&gt;, `18` &lt;int&gt;, `19` &lt;int&gt;, `20` &lt;int&gt;, `21` &lt;int&gt;,\n#   `22` &lt;int&gt;, `23` &lt;int&gt;, `24` &lt;int&gt;, `25` &lt;int&gt;, `26` &lt;int&gt;, `27` &lt;int&gt;,\n#   `28` &lt;int&gt;, `29` &lt;int&gt;, `30` &lt;int&gt;, `31` &lt;int&gt;, `32` &lt;int&gt;, `33` &lt;int&gt;,\n#   `34` &lt;int&gt;, `35` &lt;int&gt;, `36` &lt;int&gt;, `37` &lt;int&gt;, `38` &lt;int&gt;, `39` &lt;int&gt;,\n#   `40` &lt;int&gt;, `41` &lt;int&gt;, `42` &lt;int&gt;, `43` &lt;int&gt;, `44` &lt;int&gt;, `45` &lt;int&gt;,\n#   `46` &lt;int&gt;, `47` &lt;int&gt;, `48` &lt;int&gt;, `49` &lt;int&gt;, `50` &lt;int&gt;, `51` &lt;int&gt;, …\n\n\n\n\n\n\n\nfiltered_snps &lt;- minor_alleles$SNP_ID  # SNPs with nonzero MAF\n\n# Remove columns that are no longer in minor_allele_counts due to filtering\nvalid_columns &lt;- intersect(names(minor_allele_counts), as.character(filtered_snps))\n\n# Select only the valid columns (Person_ID + SNP columns) and remove columns that are all NA\nminor_allele_counts &lt;- minor_allele_counts %&gt;%\n  select(Person_ID, all_of(valid_columns)) %&gt;%\n  select(where(~ !all(is.na(.))))\n\nhead(minor_allele_counts)\n\n# A tibble: 6 × 227,591\n  Person_ID   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1         1     0     1     1     1     1     1     0     1     2     1     2\n2         2     0     0     1     2     1     2     1     1     1     1     1\n3         3     0     0     0     1     1     0     1     0     1     1     2\n4         4     0     0     0     1     0     1     1     1     1     0     0\n5         5     0     0     0     0     1     0     2     0     1     1     1\n6         6     0     0     0     0     1     2     1     0     1     1     1\n# ℹ 227,579 more variables: `12` &lt;int&gt;, `13` &lt;int&gt;, `14` &lt;int&gt;, `15` &lt;int&gt;,\n#   `16` &lt;int&gt;, `17` &lt;int&gt;, `18` &lt;int&gt;, `19` &lt;int&gt;, `20` &lt;int&gt;, `21` &lt;int&gt;,\n#   `22` &lt;int&gt;, `23` &lt;int&gt;, `24` &lt;int&gt;, `25` &lt;int&gt;, `26` &lt;int&gt;, `27` &lt;int&gt;,\n#   `28` &lt;int&gt;, `29` &lt;int&gt;, `30` &lt;int&gt;, `31` &lt;int&gt;, `32` &lt;int&gt;, `33` &lt;int&gt;,\n#   `34` &lt;int&gt;, `35` &lt;int&gt;, `36` &lt;int&gt;, `37` &lt;int&gt;, `38` &lt;int&gt;, `39` &lt;int&gt;,\n#   `40` &lt;int&gt;, `41` &lt;int&gt;, `42` &lt;int&gt;, `43` &lt;int&gt;, `44` &lt;int&gt;, `45` &lt;int&gt;,\n#   `46` &lt;int&gt;, `47` &lt;int&gt;, `48` &lt;int&gt;, `49` &lt;int&gt;, `50` &lt;int&gt;, `51` &lt;int&gt;, …\n\n\n\n\n\nBonferroni solution: One very widely used approach for adjusting for multiple testing is known as the Bonferroni Correction. We calculate our new significance threshold by dividing our desired family-wise error rate by the number of hypothesis tests that we conducted.\nSignificance threshold = 0.05 / 227642 = 2.196431e-07\n\n\n\n\n\n\nminor_allele_counts &lt;- minor_allele_counts %&gt;% \n  mutate(IQ = `160000`*15 + rnorm(n(), mean = 100, sd = 15))\n\n\n\n\n\n\nIQ &lt;- minor_allele_counts$IQ\nX &lt;- as.matrix(minor_allele_counts %&gt;% select(-IQ, -Person_ID))  # Convert predictors to a matrix\n\n# Initialize vectors to store results\nnum_snps &lt;- ncol(X)\nall_betas &lt;- numeric(num_snps)\nall_ses &lt;- numeric(num_snps)\nall_tstats &lt;- numeric(num_snps)\nall_pvals &lt;- numeric(num_snps)\n\n# Run marginal regressions efficiently\nfor (i in 1:num_snps) {\n  model &lt;- lm(IQ ~ X[, i])  # Fit a simple regression model for each predictor\n  \n  coefinfo &lt;- tidy(model)  # Extract coefficients\n  \n  all_betas[i] &lt;- coefinfo$estimate[2]\n  all_ses[i] &lt;- coefinfo$std.error[2]\n  all_tstats[i] &lt;- coefinfo$statistic[2]\n  all_pvals[i] &lt;- coefinfo$p.value[2]\n  \n  if (i %% 10000 == 0) print(paste(\"Analyzing predictor\", i))  # Progress update\n}\n\n[1] \"Analyzing predictor 10000\"\n[1] \"Analyzing predictor 20000\"\n[1] \"Analyzing predictor 30000\"\n[1] \"Analyzing predictor 40000\"\n[1] \"Analyzing predictor 50000\"\n[1] \"Analyzing predictor 60000\"\n[1] \"Analyzing predictor 70000\"\n[1] \"Analyzing predictor 80000\"\n[1] \"Analyzing predictor 90000\"\n[1] \"Analyzing predictor 100000\"\n[1] \"Analyzing predictor 110000\"\n[1] \"Analyzing predictor 120000\"\n[1] \"Analyzing predictor 130000\"\n[1] \"Analyzing predictor 140000\"\n[1] \"Analyzing predictor 150000\"\n[1] \"Analyzing predictor 160000\"\n[1] \"Analyzing predictor 170000\"\n[1] \"Analyzing predictor 180000\"\n[1] \"Analyzing predictor 190000\"\n[1] \"Analyzing predictor 200000\"\n[1] \"Analyzing predictor 210000\"\n[1] \"Analyzing predictor 220000\"\n\n# Create results data frame\nresults &lt;- tibble(\n  SNP_ID = as.integer(colnames(X)),\n  Beta = all_betas,\n  SE = all_ses,\n  T_Stat = all_tstats,\n  P_Value = all_pvals\n)\n\n# View top predictors by significance\nresults %&gt;% arrange(P_Value) %&gt;% head(10)\n\n# A tibble: 10 × 5\n   SNP_ID   Beta    SE T_Stat  P_Value\n    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 160000  16.5   2.39   6.89 5.36e-10\n 2  38818  10.8   2.20   4.91 3.65e- 6\n 3 208474 -11.8   2.46  -4.80 5.71e- 6\n 4 158171 -16.2   3.45  -4.71 8.14e- 6\n 5 222009  11.0   2.41   4.55 1.52e- 5\n 6  11168 -23.4   5.30  -4.41 2.61e- 5\n 7  26919   9.75  2.24   4.34 3.42e- 5\n 8  89736  10.3   2.38   4.30 4.02e- 5\n 9  39087 -10.2   2.37  -4.30 4.04e- 5\n10  98639  10.8   2.53   4.28 4.35e- 5\n\n\n\n# Double checking that it is correct\ntidy(lm(IQ ~ `1726`, data = minor_allele_counts))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  108.         3.07    35.1   2.35e-57\n2 `1726`        -0.333      2.66    -0.125 9.01e- 1\n\n\n\n\n\n\nfinal_results &lt;- minor_alleles %&gt;% \n  left_join(results, by=\"SNP_ID\") %&gt;% \n  left_join(snp_grid, by=\"SNP_ID\") %&gt;% \n  select(SNP_ID, Minor_Allele, MAF, Chromosome, Position, P_Value)\n\n\n\n\n\nfinal_results %&gt;% \n  mutate(Chromosome = as.factor(`Chromosome`)) %&gt;% \n  ggplot(aes(x = SNP_ID, y = -log10(P_Value), color = Chromosome))+\n  geom_point()+\n  geom_hline(yintercept = -log10(2.196431e-07), linetype = \"dashed\", color = \"Pink\")\n\n\n\n\n\n\n\n\n\nqq(final_results$P_Value, main = \"Q-Q plot of GWAS p-values\", xlim = c(0, 7), ylim = c(0,\n    12), pch = 18, col = \"blue4\", cex = 1, las = 1)"
  },
  {
    "objectID": "projects/SP5/ContentSummary_YesAssoc.html#step-1-number-of-observations",
    "href": "projects/SP5/ContentSummary_YesAssoc.html#step-1-number-of-observations",
    "title": "Content Summary #1",
    "section": "",
    "text": "100 observations 200,000 SNP’s"
  },
  {
    "objectID": "projects/SP5/ContentSummary_YesAssoc.html#step-2-construct-genotype-for-n-observations",
    "href": "projects/SP5/ContentSummary_YesAssoc.html#step-2-construct-genotype-for-n-observations",
    "title": "Content Summary #1",
    "section": "",
    "text": "In each position(SNP) an observation either gets a AG, GA, AA, GG, each A/G represents what they got from each parent in that position.\n\n\n\nset.seed(42)\n\n# Number of people, SNPs per chromosome, and chromosomes\nnum_people &lt;- 100\nnum_snps_per_chr &lt;- 10000\nnum_chromosomes &lt;- 23\n\nsnp_grid &lt;- expand_grid(\n  Chromosome = 1:num_chromosomes,\n  Position = 1:num_snps_per_chr\n) %&gt;%\n  mutate(\n    SNP_ID = row_number(),\n    prob_A = runif(n(), 0, 1)\n  ) %&gt;%\n  mutate(prob_G = 1 - prob_A) \n\nhead(snp_grid)\n\n# A tibble: 6 × 5\n  Chromosome Position SNP_ID prob_A prob_G\n       &lt;int&gt;    &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1          1        1      1  0.915 0.0852\n2          1        2      2  0.937 0.0629\n3          1        3      3  0.286 0.714 \n4          1        4      4  0.830 0.170 \n5          1        5      5  0.642 0.358 \n6          1        6      6  0.519 0.481 \n\n\n\n\n\n\n# Create person data with SNPs and varying probabilities\npeople_data &lt;- tibble(Person_ID = 1:num_people) %&gt;%\n  crossing(snp_grid) %&gt;%\n  mutate(\n    Allele_1 = ifelse(runif(n()) &lt; prob_A, \"A\", \"G\"),\n    Allele_2 = ifelse(runif(n()) &lt; prob_A, \"A\", \"G\")\n  )\n\nhead(people_data)\n\n# A tibble: 6 × 8\n  Person_ID Chromosome Position SNP_ID prob_A prob_G Allele_1 Allele_2\n      &lt;int&gt;      &lt;int&gt;    &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n1         1          1        1      1  0.915 0.0852 A        A       \n2         1          1        2      2  0.937 0.0629 A        G       \n3         1          1        3      3  0.286 0.714  A        G       \n4         1          1        4      4  0.830 0.170  G        A       \n5         1          1        5      5  0.642 0.358  A        G       \n6         1          1        6      6  0.519 0.481  A        G       \n\n\n\n\n\n\n# Identify the minor allele for each SNP\nminor_alleles &lt;- people_data %&gt;%\n  group_by(SNP_ID) %&gt;%\n  mutate(\n    count_A = sum(Allele_1 == \"A\" | Allele_2 == \"A\"), \n    count_G = sum(Allele_1 == \"G\" | Allele_2 == \"G\")\n  ) %&gt;%\n  mutate(\n    Minor_Allele = if_else(count_A &lt; count_G, \"A\", \"G\"), \n    MAF = pmin(count_A, count_G) / (2 * num_people)\n  ) %&gt;%\n  select(SNP_ID, Minor_Allele, MAF) %&gt;%\n  distinct()\n\n# NOTE: This is part of quality control\nminor_alleles &lt;- minor_alleles %&gt;% \n  filter(MAF != 0)\n\nhead(minor_alleles)\n\n# A tibble: 6 × 3\n# Groups:   SNP_ID [6]\n  SNP_ID Minor_Allele   MAF\n   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1      1 G            0.075\n2      2 G            0.09 \n3      3 A            0.235\n4      4 G            0.19 \n5      5 G            0.3  \n6      6 A            0.35 \n\n\n\n\n\n\n# Counting the minor alleles at each SNP position\nminor_allele_counts &lt;- people_data %&gt;%\n  left_join(minor_alleles, by = \"SNP_ID\") %&gt;%\n  mutate(\n    Minor_Allele_Count = \n      (Minor_Allele == Allele_1) + (Minor_Allele == Allele_2)  # Vectorized count\n  ) %&gt;%\n  select(Person_ID, SNP_ID, Minor_Allele_Count) %&gt;%  # Keep only necessary columns\n  pivot_wider(names_from = SNP_ID, values_from = Minor_Allele_Count)\n\nhead(minor_allele_counts)\n\n# A tibble: 6 × 230,001\n  Person_ID   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1         1     0     1     1     1     1     1     0     1     2     1     2\n2         2     0     0     1     2     1     2     1     1     1     1     1\n3         3     0     0     0     1     1     0     1     0     1     1     2\n4         4     0     0     0     1     0     1     1     1     1     0     0\n5         5     0     0     0     0     1     0     2     0     1     1     1\n6         6     0     0     0     0     1     2     1     0     1     1     1\n# ℹ 229,989 more variables: `12` &lt;int&gt;, `13` &lt;int&gt;, `14` &lt;int&gt;, `15` &lt;int&gt;,\n#   `16` &lt;int&gt;, `17` &lt;int&gt;, `18` &lt;int&gt;, `19` &lt;int&gt;, `20` &lt;int&gt;, `21` &lt;int&gt;,\n#   `22` &lt;int&gt;, `23` &lt;int&gt;, `24` &lt;int&gt;, `25` &lt;int&gt;, `26` &lt;int&gt;, `27` &lt;int&gt;,\n#   `28` &lt;int&gt;, `29` &lt;int&gt;, `30` &lt;int&gt;, `31` &lt;int&gt;, `32` &lt;int&gt;, `33` &lt;int&gt;,\n#   `34` &lt;int&gt;, `35` &lt;int&gt;, `36` &lt;int&gt;, `37` &lt;int&gt;, `38` &lt;int&gt;, `39` &lt;int&gt;,\n#   `40` &lt;int&gt;, `41` &lt;int&gt;, `42` &lt;int&gt;, `43` &lt;int&gt;, `44` &lt;int&gt;, `45` &lt;int&gt;,\n#   `46` &lt;int&gt;, `47` &lt;int&gt;, `48` &lt;int&gt;, `49` &lt;int&gt;, `50` &lt;int&gt;, `51` &lt;int&gt;, …"
  },
  {
    "objectID": "projects/SP5/ContentSummary_YesAssoc.html#step-3-quality-control",
    "href": "projects/SP5/ContentSummary_YesAssoc.html#step-3-quality-control",
    "title": "Content Summary #1",
    "section": "",
    "text": "filtered_snps &lt;- minor_alleles$SNP_ID  # SNPs with nonzero MAF\n\n# Remove columns that are no longer in minor_allele_counts due to filtering\nvalid_columns &lt;- intersect(names(minor_allele_counts), as.character(filtered_snps))\n\n# Select only the valid columns (Person_ID + SNP columns) and remove columns that are all NA\nminor_allele_counts &lt;- minor_allele_counts %&gt;%\n  select(Person_ID, all_of(valid_columns)) %&gt;%\n  select(where(~ !all(is.na(.))))\n\nhead(minor_allele_counts)\n\n# A tibble: 6 × 227,591\n  Person_ID   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1         1     0     1     1     1     1     1     0     1     2     1     2\n2         2     0     0     1     2     1     2     1     1     1     1     1\n3         3     0     0     0     1     1     0     1     0     1     1     2\n4         4     0     0     0     1     0     1     1     1     1     0     0\n5         5     0     0     0     0     1     0     2     0     1     1     1\n6         6     0     0     0     0     1     2     1     0     1     1     1\n# ℹ 227,579 more variables: `12` &lt;int&gt;, `13` &lt;int&gt;, `14` &lt;int&gt;, `15` &lt;int&gt;,\n#   `16` &lt;int&gt;, `17` &lt;int&gt;, `18` &lt;int&gt;, `19` &lt;int&gt;, `20` &lt;int&gt;, `21` &lt;int&gt;,\n#   `22` &lt;int&gt;, `23` &lt;int&gt;, `24` &lt;int&gt;, `25` &lt;int&gt;, `26` &lt;int&gt;, `27` &lt;int&gt;,\n#   `28` &lt;int&gt;, `29` &lt;int&gt;, `30` &lt;int&gt;, `31` &lt;int&gt;, `32` &lt;int&gt;, `33` &lt;int&gt;,\n#   `34` &lt;int&gt;, `35` &lt;int&gt;, `36` &lt;int&gt;, `37` &lt;int&gt;, `38` &lt;int&gt;, `39` &lt;int&gt;,\n#   `40` &lt;int&gt;, `41` &lt;int&gt;, `42` &lt;int&gt;, `43` &lt;int&gt;, `44` &lt;int&gt;, `45` &lt;int&gt;,\n#   `46` &lt;int&gt;, `47` &lt;int&gt;, `48` &lt;int&gt;, `49` &lt;int&gt;, `50` &lt;int&gt;, `51` &lt;int&gt;, …"
  },
  {
    "objectID": "projects/SP5/ContentSummary_YesAssoc.html#step-4-multiple-testing-consideration",
    "href": "projects/SP5/ContentSummary_YesAssoc.html#step-4-multiple-testing-consideration",
    "title": "Content Summary #1",
    "section": "",
    "text": "Bonferroni solution: One very widely used approach for adjusting for multiple testing is known as the Bonferroni Correction. We calculate our new significance threshold by dividing our desired family-wise error rate by the number of hypothesis tests that we conducted.\nSignificance threshold = 0.05 / 227642 = 2.196431e-07"
  },
  {
    "objectID": "projects/SP5/ContentSummary_YesAssoc.html#step-5-create-quantitative-outcome-variable",
    "href": "projects/SP5/ContentSummary_YesAssoc.html#step-5-create-quantitative-outcome-variable",
    "title": "Content Summary #1",
    "section": "",
    "text": "minor_allele_counts &lt;- minor_allele_counts %&gt;% \n  mutate(IQ = `160000`*15 + rnorm(n(), mean = 100, sd = 15))"
  },
  {
    "objectID": "projects/SP5/ContentSummary_YesAssoc.html#step-6-run-marginal-regression",
    "href": "projects/SP5/ContentSummary_YesAssoc.html#step-6-run-marginal-regression",
    "title": "Content Summary #1",
    "section": "",
    "text": "IQ &lt;- minor_allele_counts$IQ\nX &lt;- as.matrix(minor_allele_counts %&gt;% select(-IQ, -Person_ID))  # Convert predictors to a matrix\n\n# Initialize vectors to store results\nnum_snps &lt;- ncol(X)\nall_betas &lt;- numeric(num_snps)\nall_ses &lt;- numeric(num_snps)\nall_tstats &lt;- numeric(num_snps)\nall_pvals &lt;- numeric(num_snps)\n\n# Run marginal regressions efficiently\nfor (i in 1:num_snps) {\n  model &lt;- lm(IQ ~ X[, i])  # Fit a simple regression model for each predictor\n  \n  coefinfo &lt;- tidy(model)  # Extract coefficients\n  \n  all_betas[i] &lt;- coefinfo$estimate[2]\n  all_ses[i] &lt;- coefinfo$std.error[2]\n  all_tstats[i] &lt;- coefinfo$statistic[2]\n  all_pvals[i] &lt;- coefinfo$p.value[2]\n  \n  if (i %% 10000 == 0) print(paste(\"Analyzing predictor\", i))  # Progress update\n}\n\n[1] \"Analyzing predictor 10000\"\n[1] \"Analyzing predictor 20000\"\n[1] \"Analyzing predictor 30000\"\n[1] \"Analyzing predictor 40000\"\n[1] \"Analyzing predictor 50000\"\n[1] \"Analyzing predictor 60000\"\n[1] \"Analyzing predictor 70000\"\n[1] \"Analyzing predictor 80000\"\n[1] \"Analyzing predictor 90000\"\n[1] \"Analyzing predictor 100000\"\n[1] \"Analyzing predictor 110000\"\n[1] \"Analyzing predictor 120000\"\n[1] \"Analyzing predictor 130000\"\n[1] \"Analyzing predictor 140000\"\n[1] \"Analyzing predictor 150000\"\n[1] \"Analyzing predictor 160000\"\n[1] \"Analyzing predictor 170000\"\n[1] \"Analyzing predictor 180000\"\n[1] \"Analyzing predictor 190000\"\n[1] \"Analyzing predictor 200000\"\n[1] \"Analyzing predictor 210000\"\n[1] \"Analyzing predictor 220000\"\n\n# Create results data frame\nresults &lt;- tibble(\n  SNP_ID = as.integer(colnames(X)),\n  Beta = all_betas,\n  SE = all_ses,\n  T_Stat = all_tstats,\n  P_Value = all_pvals\n)\n\n# View top predictors by significance\nresults %&gt;% arrange(P_Value) %&gt;% head(10)\n\n# A tibble: 10 × 5\n   SNP_ID   Beta    SE T_Stat  P_Value\n    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 160000  16.5   2.39   6.89 5.36e-10\n 2  38818  10.8   2.20   4.91 3.65e- 6\n 3 208474 -11.8   2.46  -4.80 5.71e- 6\n 4 158171 -16.2   3.45  -4.71 8.14e- 6\n 5 222009  11.0   2.41   4.55 1.52e- 5\n 6  11168 -23.4   5.30  -4.41 2.61e- 5\n 7  26919   9.75  2.24   4.34 3.42e- 5\n 8  89736  10.3   2.38   4.30 4.02e- 5\n 9  39087 -10.2   2.37  -4.30 4.04e- 5\n10  98639  10.8   2.53   4.28 4.35e- 5\n\n\n\n# Double checking that it is correct\ntidy(lm(IQ ~ `1726`, data = minor_allele_counts))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  108.         3.07    35.1   2.35e-57\n2 `1726`        -0.333      2.66    -0.125 9.01e- 1"
  },
  {
    "objectID": "projects/SP5/ContentSummary_YesAssoc.html#step-7-one-final-dataset",
    "href": "projects/SP5/ContentSummary_YesAssoc.html#step-7-one-final-dataset",
    "title": "Content Summary #1",
    "section": "",
    "text": "final_results &lt;- minor_alleles %&gt;% \n  left_join(results, by=\"SNP_ID\") %&gt;% \n  left_join(snp_grid, by=\"SNP_ID\") %&gt;% \n  select(SNP_ID, Minor_Allele, MAF, Chromosome, Position, P_Value)"
  },
  {
    "objectID": "projects/SP5/ContentSummary_YesAssoc.html#step-8-create-visualizations",
    "href": "projects/SP5/ContentSummary_YesAssoc.html#step-8-create-visualizations",
    "title": "Content Summary #1",
    "section": "",
    "text": "final_results %&gt;% \n  mutate(Chromosome = as.factor(`Chromosome`)) %&gt;% \n  ggplot(aes(x = SNP_ID, y = -log10(P_Value), color = Chromosome))+\n  geom_point()+\n  geom_hline(yintercept = -log10(2.196431e-07), linetype = \"dashed\", color = \"Pink\")\n\n\n\n\n\n\n\n\n\nqq(final_results$P_Value, main = \"Q-Q plot of GWAS p-values\", xlim = c(0, 7), ylim = c(0,\n    12), pch = 18, col = \"blue4\", cex = 1, las = 1)"
  },
  {
    "objectID": "data_viz.html",
    "href": "data_viz.html",
    "title": "Visualizing data",
    "section": "",
    "text": "I have developed a strong passion for visualizing data. I feel as though lots can be learned about data through simple or complex data visualizations, and these same visualizations can guide a persons decision making about a problem involving data. I also am convinced that exploring data through visualizations is more appealing and can tell interesting stories."
  },
  {
    "objectID": "projects/SP2/Final_Blog_Post.html",
    "href": "projects/SP2/Final_Blog_Post.html",
    "title": "Exploration of Premier League SPI Ratings",
    "section": "",
    "text": "Motivation\n\nBeing avid viewers of the English Premier League, the top flight of English soccer and arguably the most competitive soccer league in the world, we were interested in exploring how the dominance of some teams in the league can be explained as well as how financial means contributes to this dynamic. Looking at recent articles published by the popular sports news outlet The Athletic, we found our inspiration for exploring these relationships through data. The articles we found look at how financial fair play contributes to the current landscape of the league and how some teams have abused their financial means for success over other teams(1), as well as how the league has seemingly lost a competitive edge due to the relative dominance of a couple of teams in recent years(2). These complex findings served as motivation for us to look at how the power index of teams has changed over the years, and what the relationship to financial means is with a given teams power index.\n\n\n\nIntroduction/Background\n\nThis dataset contained match results and associated “power rankings” through a Soccer Power Index (SPI) criteria across the top five European soccer leagues from the 2016-2022 seasons. For our project we decided to narrow down to just the English Premier League, the top flight of soccer in England. As avid soccer fans, this project was interesting for us to explore in a data setting because of the variety of metrics to explore that are not usually easily visible when watching a match. For example, we could look at how across multiple seasons, a team’s SPI increased or decreased and how many games the team won or lost. In addition to this, it is important to understand that in the English Premier League, teams finishing in the bottom three of the table at the conclusion of the season are relegated. This adds an interesting dynamic to explore with the SPI variable. Despite the variety of paths we could choose with this data, we decided to take an in-depth look at how the SPI metric impacted games and teams. Do teams with higher SPI scores always win more games in a season? Do teams with lower SPI scores win fewer games? Do teams with a higher average SPI score ever drop below the average SPI threshold across all teams?\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\n\nWe started this project by delving into the losing percentages for all Premier League across 7 seasons (Figure 1). This showed us that there are teams which on average, performed far worse than others across all seasons. For example, teams such as Norwich City, Sunderland, and Middlesborough are all teams with very high losing percentages in the Premier League. This makes sense as these are teams that are often at the bottom of the league and often get relegated at the end of the season. We then looked the distribution of SPI scores in the dataset to verify that we had a normal distribution of SPI scores across the dataset, with a median SPI value across all teams and seasons plotted for reference (Figure 2). This showed that the majority of SPI scores are roughly within the 60-80 range. SPI rating or score can be defined as the percentage of available points — a win is worth 3 points, a tie worth 1 point, and a loss worth 0 points — the team would be expected to take if that match were played over and over again.\n\n\n\n\nExploration of SPI\n\nFigure 3\n\n\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\n\n\n\nWe then set out to look at the relationship between cumulative wins and losses impacted SPI over 7 seasons for teams, as well as if there was a relationship between the number of goals scored in a game and mean SPI over 7 seasons. The first plot (Figure 3) showed us that there is a positive linear relationship between the cumulative number of wins a team has and their mean SPI over 7 seasons. However, the relationship between mean SPI and cumulative number of losses (Figure 4) did not show a linear relationship. Looking at total losses plotted against SPI scores, there was a bell-curve shape to the plot. This trend was likely influenced by the presence of teams who often get promoted/relegated from the English Premier League, therefore they would have a very low SPI rating but a lower number of total losses then teams who are constantly in the Premier League but arent super succesful. To illustrate this idea, here is an example. A recently promoted team in the league may lose all their games across a season, be relegated, and then never appear in the league within the timespan of this dataset. In contrast, there might be teams in the league who consistently win just enough games to stay in the league, but as a result, also have low SPI scores. This results in more accumulated losses in total than the one season teams who crashed out, leading to this unexpected trend. The plot looking at goals per game vs mean SPI rating over 7 seasons (Figure 5) revealed a similar pattern to Figure 3, in that there was a positive linear relationship between the number of goals a team scored in a game and their mean SPI over 7 seasons.\nWhat we gathered from these results is that a more effective measure of looking at team performance across all seasons is not by looking at cumulative wins or losses, but instead by looking at the relative win/loss percentage of teams.\n\n\n\nFigure 6\n\n\n\n\n\n\n\n\n\n\nNext, we wanted to look at the minimum and maximum SPI scores across all teams (Figure 6), and if teams which we know to have been historically successful have dropped below the average SPI score across all teams in the Premier League. We found that teams Arsenal, Manchester City, Manchester United, Chelsea, Liverpool, and Tottenham have not dropped below the average SPI threshold. This makes sense as they have historically been the most successful teams, and, as shown in the SPI analysis, make up the top six highest average SPI scores across the dataset. The teams that fall below the average were Cardiff City, Huddersfield Town, Hull City, Middlesborough, Norwich City, Nottingham Forest, Stoke City, Sunderland, Swansea, and West Bromwich Albion, which makes sense as these teams have been relegated at one point during the timespan of the data.\n\n\n\nFigure 7\n\n\n\n\n\n\n\n\n\n\nThen, we looked at how the transfer activity of Premier League teams has impacted the SPI of teams (Figure 7). For context, the data we scraped references 5 years worth of transfer activity. The way players get transfered from team to team is by the teams buying and selling them. Almost every team tends to spend more money buying players then they do selling players to other teams. The values we scraped represent the net gains and losses that a team made throughout the five year period, and all values we scraped (except for one) were negative which is why we labelled this figure as “net losses”. What we found is that, for the most part, teams that spent more than they made in the transfer market (higher net losses) had higher SPI, but some teams went against this generalization. For example, Newcastle has much higher net losses than any other comparable teams as far as SPI, but this is due to the club being recently bought by new ownership and new cashflow being injected into the team in terms of transfer activity. Additionally, the top two teams Liverpool and Manchester City do not have anywhere near the amount of net losses that the next top 4 teams have. This demonstrates the idea that not only is it the amount of money spent that is important, but also the overall strategy of recruitment and youth development.\n\n\n\nFigure 8\n\n\n\n\n\n\n\n\n\n\nNext, we wanted to look at how effectively the win probability variable predicted the outcome of the match winner in the dataset (Figure 8). What we found is that some teams are correctly predicted to win a match more than others. The teams which are more correctly predicted to win most of the time have a higher SPI (as seen in Figure 6),which leads us to believe that the probability variable combined with SPI is a fairly good metric of team success.\n\n\n\nWarning: No renderer available. Please install the gifski, av, or magick package to\ncreate animated output\n\n\n\nFinally, for our last visualization we decided to look at how over the course of 7 seasons SPI rating fluctuated for each team and how well it predicted for the winner of the league. What we found is that, in most cases, the winner of the league had a high mean SPI. We also found that SPI varied quite a bit across seasons for most teams, showing how dynamic the league is from year to year. The one exception to SPI being a determinant of potential league winner’s was in 2016, when Leicester City won the league. Going into that season, Leicester had just come off from a relegation battle the previous season and had odds of winning the league of 5,000 to 1, or a 0.02% chance of winning the league.\n\n\n\n\nLimitations\n\nThe limitations of this project were the range of years of available data for the different datasets that we used. The dataset containing the SPI data had only 7 seasons from 2015-2022, and it would have been great to have had data from earlier years until the present. On a similar note, the data we scraped for transfer market net losses only had 5 years worth of data from 2016-2021, and similarly it would have been nice to have had data up until the present. An additional limitation of this dataset is that there are fairly few financial regulations on teams, especially in the earlier years of the dataset, which certainly skews the results we see and produces the “top six” effect of teams dominant financially and performance-wise.\n\n\n\nMain Takeaways\n\nThis data exploration revealed that SPI is a fairly good metric of measuring a team’s performance in a season, especially when a team is in the top six of the premier league. This metric loses power as teams are lower in the premier league table, as shown in our proportion plot showing teams accurately predicted to win based on probability. Furthermore, teams with larger financial capacity for spending, or rather for having more net losses, are more likely to perform well save for a couple of outliers.\n\n\n\nFuture Directions\n\nFuture directions of this exploration could be to include some statistical tests on some of the relationships which we see. For example, can we predict with significance when a team will lose a match based on their financial capacity as well as their SPI? Or can we show statistical significance between the relationships which we displayed in this exploration? There are still many questions which can be analyzed in this dataset, and exploring the statistical power of the SPI variable has lots of potential for future research.\n\n\n\nReferences\n\n\nStaff, T. A. U. (n.d.). Premier League financial fair play rules explained. The Athletic. Retrieved April 25, 2024, from https://theathletic.com/5198425/2024/01/13/premier-league-ffp-financial-fair-play/\nEccleshare, C. (n.d.). With Manchester City’s continuing dominance, has the Premier League become a one-team division? The Athletic. Retrieved April 25, 2024, from https://theathletic.com/4520869/2023/05/16/premier-league-manchester-city-dominance/"
  },
  {
    "objectID": "projects/SP5/ContentSummary_NoAssoc.html",
    "href": "projects/SP5/ContentSummary_NoAssoc.html",
    "title": "Content Summary #1",
    "section": "",
    "text": "Genotype - Genetic makeup of an organism, or an organisms specific DNA sequence.\n\nPhenotype - Observable characteristics of an organism resulting from it’s genotype.\n\nNucleotide - The building blocks of DNA. 4 types: Adenine(A), Thymine(T), Guanine(G), Cystosine(C). Adenine and Thymine always pair together, and Guanine and Cystosine always pair together. Nucleotides play a large role in expressing genetic information. We tend to only look at oneside of the double-helix(ex: A G T C or T C A G) because we know that if an A is present on one side, the other side will be a T.\n\nGene - Specific sequence of nucleotides that code for a specific trait. Also can be thought of as a segment of DNA.\n\nGenetic Variant - Places where two humans genes differ. AT GC TA vs AT AT TA\n\nSingle nucleotide variant(SNV) - A type of genetic variant that occurs when a single nucleotide (adenine, thymine, cytosine, or guanine) in the genome sequence differs.\n\nAllele - For a given SNV, an allele is the different possible nucleotides at that position(ex: A & G or C & T). At each position you have two alleles because you inherit one from each parent.\n\nSingle Nucleotide Polymorphism(SNP) - Type of genetic variant where the minor allele has an MAF &gt; 1%. A SNP is a variation where a single nucleotide at a specific position in the DNA sequence differs between individuals.\n\nMajor Allele - More common allele in a location/variant.\n\nMinor Allele - Less common allele in a location/variant. Minor allele is found by taking the combination of both alleles in each position(one from each parent) and then added up the total for each nucleotide in position one and then total for each nucleotide in position two, and then whichever one is least common is the minor allele.\n\n\n\nPerson\nFather Allele\nMother Allele\n\n\n\n\nPerson 1\nA\nA\n\n\nPerson 2\nA\nG\n\n\nPerson 3\nG\nA\n\n\nPerson 4\nA\nA\n\n\nPerson 5\nG\nG\n\n\nPerson 6\nA\nA\n\n\n\n\nTotal A’s: 8\nTotal G’s: 4\nMinor allele: G\nMajor allele: A\n\n\n\n\n\nPerson\nMinor Alleles\n\n\n\n\nPerson 1\n0\n\n\nPerson 2\n1\n\n\nPerson 3\n1\n\n\nPerson 4\n0\n\n\nPerson 5\n2\n\n\nPerson 6\n0\n\n\n\n\n\nMinor Allele Frequency - Frequency of the minor allele in a population."
  },
  {
    "objectID": "projects/SP5/ContentSummary_NoAssoc.html#step-1-study-design",
    "href": "projects/SP5/ContentSummary_NoAssoc.html#step-1-study-design",
    "title": "Content Summary #1",
    "section": "Step 1: Study Design",
    "text": "Step 1: Study Design\nWe are going to be using 100 “people” in this study. For each person we are going to gather 230,000 SNP’s(or nucleotides). This means that for each chromosome we will analyze 10,000 positions."
  },
  {
    "objectID": "projects/SP5/ContentSummary_NoAssoc.html#step-2-construct-genotype-for-n-observations",
    "href": "projects/SP5/ContentSummary_NoAssoc.html#step-2-construct-genotype-for-n-observations",
    "title": "Content Summary #1",
    "section": "Step 2: Construct genotype for n observations",
    "text": "Step 2: Construct genotype for n observations\nIn each position(SNP) an observation either gets a AG, GA, AA, GG, each A/G represents what they got from each parent in that position(A or G from parent #1, and an A or G from parent #2).\nFirst, for each SNP(or nucleotide) we have to come up with a probability of getting an A or G as one of your alleles. The probability of getting an A is a randomly chosen number from 0 to 1, and on the flip side the probability of getting a G is 1-(probability of getting an A).\n\nset.seed(42)\n\n# Number of people, SNPs per chromosome, and chromosomes\nnum_people &lt;- 100\nnum_snps_per_chr &lt;- 10000\nnum_chromosomes &lt;- 23\n\nsnp_grid &lt;- expand_grid(\n  Chromosome = 1:num_chromosomes,\n  Position = 1:num_snps_per_chr\n) %&gt;%\n  mutate(\n    SNP_ID = row_number(),\n    prob_A = runif(n(), 0, 1)\n  ) %&gt;%\n  mutate(prob_G = 1 - prob_A) \n\nhead(snp_grid)\n\n# A tibble: 6 × 5\n  Chromosome Position SNP_ID prob_A prob_G\n       &lt;int&gt;    &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1          1        1      1  0.915 0.0852\n2          1        2      2  0.937 0.0629\n3          1        3      3  0.286 0.714 \n4          1        4      4  0.830 0.170 \n5          1        5      5  0.642 0.358 \n6          1        6      6  0.519 0.481 \n\n\nNow, using the probabilities for an A and G in each SNP position, we are going to assign everyone 2 alleles(one from each parent) at each position.\n\n# Create person data with SNPs and varying probabilities\npeople_data &lt;- tibble(Person_ID = 1:num_people) %&gt;%\n  crossing(snp_grid) %&gt;%\n  mutate(\n    Allele_1 = ifelse(runif(n()) &lt; prob_A, \"A\", \"G\"),\n    Allele_2 = ifelse(runif(n()) &lt; prob_A, \"A\", \"G\")\n  )\n\nhead(people_data)\n\n# A tibble: 6 × 8\n  Person_ID Chromosome Position SNP_ID prob_A prob_G Allele_1 Allele_2\n      &lt;int&gt;      &lt;int&gt;    &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n1         1          1        1      1  0.915 0.0852 A        A       \n2         1          1        2      2  0.937 0.0629 A        G       \n3         1          1        3      3  0.286 0.714  A        G       \n4         1          1        4      4  0.830 0.170  G        A       \n5         1          1        5      5  0.642 0.358  A        G       \n6         1          1        6      6  0.519 0.481  A        G       \n\n\nOnce everyone has two alleles at each position, we are going to count the number of each allele that everyone has at every position(ex. two A’s, one A, two G’s, etc.). Then we can add up across the whole population to figure out which allele is less common at that SNP, and we can also calculate the minor allele frequency at each SNP.\n\n# Identify the minor allele for each SNP\nminor_alleles &lt;- people_data %&gt;%\n  group_by(SNP_ID) %&gt;%\n  mutate(\n    count_A = sum(Allele_1 == \"A\" | Allele_2 == \"A\"), \n    count_G = sum(Allele_1 == \"G\" | Allele_2 == \"G\")\n  ) %&gt;%\n  mutate(\n    Minor_Allele = if_else(count_A &lt; count_G, \"A\", \"G\"), \n    MAF = pmin(count_A, count_G) / (2 * num_people)\n  ) %&gt;%\n  select(SNP_ID, Minor_Allele, MAF) %&gt;%\n  distinct()\n\n# NOTE: This is part of quality control\nminor_alleles &lt;- minor_alleles %&gt;% \n  filter(MAF != 0)\n\nhead(minor_alleles)\n\n# A tibble: 6 × 3\n# Groups:   SNP_ID [6]\n  SNP_ID Minor_Allele   MAF\n   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1      1 G            0.075\n2      2 G            0.09 \n3      3 A            0.235\n4      4 G            0.19 \n5      5 G            0.3  \n6      6 A            0.35 \n\n\nOnce we know the minor allele for each SNP, we can calculate the number of minor alleles that each person has at each position. These will become our predictor variables for our regression models.\n\n# Counting the minor alleles at each SNP position\nminor_allele_counts &lt;- people_data %&gt;%\n  left_join(minor_alleles, by = \"SNP_ID\") %&gt;%\n  mutate(\n    Minor_Allele_Count = \n      (Minor_Allele == Allele_1) + (Minor_Allele == Allele_2)  # Vectorized count\n  ) %&gt;%\n  select(Person_ID, SNP_ID, Minor_Allele_Count) %&gt;%  # Keep only necessary columns\n  pivot_wider(names_from = SNP_ID, values_from = Minor_Allele_Count)\n\nhead(minor_allele_counts)\n\n# A tibble: 6 × 230,001\n  Person_ID   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1         1     0     1     1     1     1     1     0     1     2     1     2\n2         2     0     0     1     2     1     2     1     1     1     1     1\n3         3     0     0     0     1     1     0     1     0     1     1     2\n4         4     0     0     0     1     0     1     1     1     1     0     0\n5         5     0     0     0     0     1     0     2     0     1     1     1\n6         6     0     0     0     0     1     2     1     0     1     1     1\n# ℹ 229,989 more variables: `12` &lt;int&gt;, `13` &lt;int&gt;, `14` &lt;int&gt;, `15` &lt;int&gt;,\n#   `16` &lt;int&gt;, `17` &lt;int&gt;, `18` &lt;int&gt;, `19` &lt;int&gt;, `20` &lt;int&gt;, `21` &lt;int&gt;,\n#   `22` &lt;int&gt;, `23` &lt;int&gt;, `24` &lt;int&gt;, `25` &lt;int&gt;, `26` &lt;int&gt;, `27` &lt;int&gt;,\n#   `28` &lt;int&gt;, `29` &lt;int&gt;, `30` &lt;int&gt;, `31` &lt;int&gt;, `32` &lt;int&gt;, `33` &lt;int&gt;,\n#   `34` &lt;int&gt;, `35` &lt;int&gt;, `36` &lt;int&gt;, `37` &lt;int&gt;, `38` &lt;int&gt;, `39` &lt;int&gt;,\n#   `40` &lt;int&gt;, `41` &lt;int&gt;, `42` &lt;int&gt;, `43` &lt;int&gt;, `44` &lt;int&gt;, `45` &lt;int&gt;,\n#   `46` &lt;int&gt;, `47` &lt;int&gt;, `48` &lt;int&gt;, `49` &lt;int&gt;, `50` &lt;int&gt;, `51` &lt;int&gt;, …"
  },
  {
    "objectID": "projects/SP5/ContentSummary_NoAssoc.html#step-3-quality-control",
    "href": "projects/SP5/ContentSummary_NoAssoc.html#step-3-quality-control",
    "title": "Content Summary #1",
    "section": "Step 3: Quality control",
    "text": "Step 3: Quality control\nWe want to filter out all of the SNP that dont have a minor allele, meaning everyone has the same alleles at that position.\n\nfiltered_snps &lt;- minor_alleles$SNP_ID  # SNPs with nonzero MAF\n\n# Remove columns that are no longer in minor_allele_counts due to filtering\nvalid_columns &lt;- intersect(names(minor_allele_counts), as.character(filtered_snps))\n\n# Select only the valid columns (Person_ID + SNP columns) and remove columns that are all NA\nminor_allele_counts &lt;- minor_allele_counts %&gt;%\n  select(Person_ID, all_of(valid_columns)) %&gt;%\n  select(where(~ !all(is.na(.))))\n\nhead(minor_allele_counts)\n\n# A tibble: 6 × 227,591\n  Person_ID   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`\n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1         1     0     1     1     1     1     1     0     1     2     1     2\n2         2     0     0     1     2     1     2     1     1     1     1     1\n3         3     0     0     0     1     1     0     1     0     1     1     2\n4         4     0     0     0     1     0     1     1     1     1     0     0\n5         5     0     0     0     0     1     0     2     0     1     1     1\n6         6     0     0     0     0     1     2     1     0     1     1     1\n# ℹ 227,579 more variables: `12` &lt;int&gt;, `13` &lt;int&gt;, `14` &lt;int&gt;, `15` &lt;int&gt;,\n#   `16` &lt;int&gt;, `17` &lt;int&gt;, `18` &lt;int&gt;, `19` &lt;int&gt;, `20` &lt;int&gt;, `21` &lt;int&gt;,\n#   `22` &lt;int&gt;, `23` &lt;int&gt;, `24` &lt;int&gt;, `25` &lt;int&gt;, `26` &lt;int&gt;, `27` &lt;int&gt;,\n#   `28` &lt;int&gt;, `29` &lt;int&gt;, `30` &lt;int&gt;, `31` &lt;int&gt;, `32` &lt;int&gt;, `33` &lt;int&gt;,\n#   `34` &lt;int&gt;, `35` &lt;int&gt;, `36` &lt;int&gt;, `37` &lt;int&gt;, `38` &lt;int&gt;, `39` &lt;int&gt;,\n#   `40` &lt;int&gt;, `41` &lt;int&gt;, `42` &lt;int&gt;, `43` &lt;int&gt;, `44` &lt;int&gt;, `45` &lt;int&gt;,\n#   `46` &lt;int&gt;, `47` &lt;int&gt;, `48` &lt;int&gt;, `49` &lt;int&gt;, `50` &lt;int&gt;, `51` &lt;int&gt;, …"
  },
  {
    "objectID": "projects/SP5/ContentSummary_NoAssoc.html#step-4-multiple-testing-consideration",
    "href": "projects/SP5/ContentSummary_NoAssoc.html#step-4-multiple-testing-consideration",
    "title": "Content Summary #1",
    "section": "Step 4: Multiple testing consideration",
    "text": "Step 4: Multiple testing consideration\nWe need to come up with a new significance threshold. Because we are running so many regression models we want to avoid getting too many false positives. For example, if we had a significance threshold of \\(\\alpha\\) = 0.05(which is the norm in many fields) the probability of getting at least one type one error(or false positive): P(At least one T1E) = 1 - P(no T1E’s) = 1 - (1- \\(\\alpha\\) ) ^ 230000 \\(\\approx\\) 1. This means that if we were to use a significance threshold of \\(\\alpha\\) = 0.05 for 230,000 tests, we are almost guaranteed to get at least one false positive.\n\nBonferroni solution: One very widely used approach for adjusting for multiple testing is known as the Bonferroni Correction. This is one way to help keep our the probability of at least one type 1 error(AKA. Family wise error rate) low. We calculate our new significance threshold by dividing our desired family-wise error rate by the number of hypothesis tests that we conducted.\n\n\nSignificance threshold (\\(\\alpha\\))= 0.05 / 227642 = 2.196431e-07\n\nP(at least one T1E) = 1 - (1 - 2.196431e-07) ^ 230000 \\(\\approx\\) 0.05\n\nSo with this significance threshold we can keep our family wise error rate close to 0.05."
  },
  {
    "objectID": "projects/SP5/ContentSummary_NoAssoc.html#step-5-create-quantitative-outcome-variable",
    "href": "projects/SP5/ContentSummary_NoAssoc.html#step-5-create-quantitative-outcome-variable",
    "title": "Content Summary #1",
    "section": "Step 5: Create quantitative outcome variable",
    "text": "Step 5: Create quantitative outcome variable\nNow, we are going to create a random outcome variable. In this case, everyone is going to receive a fake IQ score that is randomly chosen from a distribution centered at 100.\n\nminor_allele_counts &lt;- minor_allele_counts %&gt;% \n  mutate(IQ = rnorm(n(), mean = 100, sd = 15))"
  },
  {
    "objectID": "projects/SP5/ContentSummary_NoAssoc.html#step-6-run-marginal-regression",
    "href": "projects/SP5/ContentSummary_NoAssoc.html#step-6-run-marginal-regression",
    "title": "Content Summary #1",
    "section": "Step 6: Run marginal regression",
    "text": "Step 6: Run marginal regression\nNow, using the count for minor alleles at each SNP, we are going to create 230,000 regression models that estimate IQ score from minor allele counts at each SNP. The regression formula for each SNP will look something like:\n\n\\(E[IQ | \\text{Minor Allele Count}] = \\beta_0 + \\beta_1 \\cdot \\text{(Number of Minor Alleles at Certain SNP)}\\)\n\nIQ &lt;- minor_allele_counts$IQ\nX &lt;- as.matrix(minor_allele_counts %&gt;% select(-IQ, -Person_ID))  # Convert predictors to a matrix\n\n# Initialize vectors to store results\nnum_snps &lt;- ncol(X)\nall_betas &lt;- numeric(num_snps)\nall_ses &lt;- numeric(num_snps)\nall_tstats &lt;- numeric(num_snps)\nall_pvals &lt;- numeric(num_snps)\n\n# Run marginal regressions efficiently\nfor (i in 1:num_snps) {\n  model &lt;- lm(IQ ~ X[, i])  # Fit a simple regression model for each predictor\n  \n  coefinfo &lt;- tidy(model)  # Extract coefficients\n  \n  all_betas[i] &lt;- coefinfo$estimate[2]\n  all_ses[i] &lt;- coefinfo$std.error[2]\n  all_tstats[i] &lt;- coefinfo$statistic[2]\n  all_pvals[i] &lt;- coefinfo$p.value[2]\n  \n  if (i %% 10000 == 0) print(paste(\"Analyzing predictor\", i))  # Progress update\n}\n\n[1] \"Analyzing predictor 10000\"\n[1] \"Analyzing predictor 20000\"\n[1] \"Analyzing predictor 30000\"\n[1] \"Analyzing predictor 40000\"\n[1] \"Analyzing predictor 50000\"\n[1] \"Analyzing predictor 60000\"\n[1] \"Analyzing predictor 70000\"\n[1] \"Analyzing predictor 80000\"\n[1] \"Analyzing predictor 90000\"\n[1] \"Analyzing predictor 100000\"\n[1] \"Analyzing predictor 110000\"\n[1] \"Analyzing predictor 120000\"\n[1] \"Analyzing predictor 130000\"\n[1] \"Analyzing predictor 140000\"\n[1] \"Analyzing predictor 150000\"\n[1] \"Analyzing predictor 160000\"\n[1] \"Analyzing predictor 170000\"\n[1] \"Analyzing predictor 180000\"\n[1] \"Analyzing predictor 190000\"\n[1] \"Analyzing predictor 200000\"\n[1] \"Analyzing predictor 210000\"\n[1] \"Analyzing predictor 220000\"\n\n# Create results data frame\nresults &lt;- tibble(\n  SNP_ID = as.integer(colnames(X)),\n  Beta = all_betas,\n  SE = all_ses,\n  T_Stat = all_tstats,\n  P_Value = all_pvals\n)\n\n# View top predictors by significance\nresults %&gt;% arrange(P_Value) %&gt;% head(10)\n\n# A tibble: 10 × 5\n   SNP_ID   Beta    SE T_Stat    P_Value\n    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1  39087  -9.05  1.92  -4.72 0.00000794\n 2 120849  12.9   2.73   4.71 0.00000804\n 3  37580  10.4   2.23   4.69 0.00000897\n 4  38818   8.51  1.83   4.65 0.0000103 \n 5  82446  12.5   2.73   4.59 0.0000132 \n 6  78227   8.59  1.89   4.53 0.0000164 \n 7 208474  -9.16  2.05  -4.47 0.0000214 \n 8  91032 -10.3   2.30  -4.46 0.0000219 \n 9  84519  18.6   4.19   4.44 0.0000234 \n10  70805  10.7   2.42   4.44 0.0000240 \n\n\n\n# Double checking that it is correct\ntidy(lm(IQ ~ `1726`, data = minor_allele_counts))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   104.        2.49     41.5  5.50e-64\n2 `1726`         -3.35      2.16     -1.55 1.24e- 1"
  },
  {
    "objectID": "projects/SP5/ContentSummary_NoAssoc.html#step-7-one-final-dataset",
    "href": "projects/SP5/ContentSummary_NoAssoc.html#step-7-one-final-dataset",
    "title": "Content Summary #1",
    "section": "Step 7: One final dataset",
    "text": "Step 7: One final dataset\n\nfinal_results &lt;- minor_alleles %&gt;% \n  left_join(results, by=\"SNP_ID\") %&gt;% \n  left_join(snp_grid, by=\"SNP_ID\") %&gt;% \n  select(SNP_ID, Minor_Allele, MAF, Chromosome, Position, P_Value)"
  },
  {
    "objectID": "projects/SP5/ContentSummary_NoAssoc.html#step-8-create-visualizationsmanhattan-plotusing-ggplot-qq-plot",
    "href": "projects/SP5/ContentSummary_NoAssoc.html#step-8-create-visualizationsmanhattan-plotusing-ggplot-qq-plot",
    "title": "Content Summary #1",
    "section": "Step 8: Create visualizations(Manhattan Plot(Using “ggplot”) + QQ Plot)",
    "text": "Step 8: Create visualizations(Manhattan Plot(Using “ggplot”) + QQ Plot)\n\nfinal_results %&gt;% \n  mutate(Chromosome = as.factor(`Chromosome`)) %&gt;% \n  ggplot(aes(x = SNP_ID, y = -log10(P_Value), color = Chromosome))+\n  geom_point()+\n  geom_hline(yintercept = -log10(2.196431e-07), linetype = \"dashed\", color = \"Pink\")\n\n\n\n\n\n\n\n\n\nqq(final_results$P_Value, main = \"Q-Q plot of GWAS p-values\", xlim = c(0, 7), ylim = c(0,\n    12), pch = 18, col = \"blue4\", cex = 1, las = 1)"
  },
  {
    "objectID": "projects/SP4/Final_Phase1.html",
    "href": "projects/SP4/Final_Phase1.html",
    "title": "Final Project - Phase 1",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggmap)\n#Loading Data sets\nfast_food_locations &lt;- read_csv(\"fast_food_locations.csv\")\nmenu_stat &lt;-  read_csv(\"menu_items.csv\") #binary variable --&gt; 'shareable' and 'limited_time_offer'\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)"
  },
  {
    "objectID": "projects/SP4/Final_Phase1.html#numerical-summarycount-of-restaurants-in-each-data-set",
    "href": "projects/SP4/Final_Phase1.html#numerical-summarycount-of-restaurants-in-each-data-set",
    "title": "Final Project - Phase 1",
    "section": "Numerical Summary(Count of restaurants in each data set)",
    "text": "Numerical Summary(Count of restaurants in each data set)\n\n# we will have to filter this whole data set so that we only get the restaurants that we want to use. \nfast_food_locations %&gt;%  #first three pages of restaurants here are all in the 'menu_items' data set below\n  group_by(name) %&gt;% \n  summarize(count = n()) %&gt;% \n  arrange(desc(count)) %&gt;% \n  filter(count &gt;= 42) \n\n# A tibble: 30 × 2\n   name            count\n   &lt;chr&gt;           &lt;int&gt;\n 1 McDonald's       1898\n 2 Taco Bell        1032\n 3 Burger King       833\n 4 Subway            776\n 5 Arby's            663\n 6 Wendy's           628\n 7 Jack in the Box   330\n 8 Pizza Hut         230\n 9 Dairy Queen       218\n10 Domino's Pizza    215\n# ℹ 20 more rows\n\n# might want to filter out the drinks, shareable items, etc. \nmenu_stat %&gt;%  \n  group_by(restaurant) %&gt;% \n  summarize(count = n()) %&gt;% \n  arrange(desc(count))\n\n# A tibble: 96 × 2\n   restaurant         count\n   &lt;chr&gt;              &lt;int&gt;\n 1 Starbucks           4504\n 2 Dunkin' Donuts      3707\n 3 Wawa                2685\n 4 Sheetz              2429\n 5 Sonic               1605\n 6 Jersey Mike's Subs  1594\n 7 Golden Corral       1395\n 8 Pizza Hut           1217\n 9 Papa John's         1207\n10 Firehouse Subs      1156\n# ℹ 86 more rows"
  },
  {
    "objectID": "projects/SP4/Final_Phase1.html#numerical-summarymean-nutritional-facts-for-each-restaurant",
    "href": "projects/SP4/Final_Phase1.html#numerical-summarymean-nutritional-facts-for-each-restaurant",
    "title": "Final Project - Phase 1",
    "section": "Numerical Summary(Mean nutritional facts for each restaurant)",
    "text": "Numerical Summary(Mean nutritional facts for each restaurant)\n\n#We could make a healthy/non-healthy binary variable, but that's relative. \naverage_nutritionalFacts_ForRestaurants &lt;- menu_stat %&gt;%  #might want to filter only the things that are not shareable, because the shareable items will probably make the averages go up because they are larger.\n  select(Year, restaurant, Item_Name, Restaurant_Item_Name, Food_Category, Calories, Total_Fat, Saturated_Fat, Trans_Fat, Cholesterol, Sodium, Potassium, Carbohydrates, Protein, Sugar, Limited_Time_Offer, Shareable) %&gt;% \n  #CODE TO FILTER OUT NA'S\n  filter(!is.na(Calories), !is.na(Total_Fat), !is.na(Saturated_Fat), !is.na(Trans_Fat), !is.na(Cholesterol), !is.na(Sodium), !is.na(Carbohydrates), !is.na(Protein), !is.na(Sugar)) %&gt;% \n  group_by(restaurant) %&gt;% \n  summarise(cal = mean(Calories), Tfat = mean(Total_Fat), Sfat = mean(Saturated_Fat), TRfat = mean(Trans_Fat), Chol = mean(Cholesterol), Sod = mean(Sodium), Carb = mean(Carbohydrates), Prot = mean(Protein), Sug = mean(Sugar)) %&gt;% \n  arrange(desc(cal))\n\nhead(average_nutritionalFacts_ForRestaurants, 12)\n\n# A tibble: 12 × 10\n   restaurant               cal  Tfat  Sfat  TRfat  Chol   Sod  Carb  Prot   Sug\n   &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Hooters                1080.  62.4 17.8  0.671  247.  1990.  69.6  60.9 11.0 \n 2 Sbarro                 1033.  45.9 16.9  0.399  134.  2365. 114.   42.6 31.4 \n 3 Famous Dave's           996.  59.9 17.7  0.882  201.  2744.  64.1  55.4 24.4 \n 4 O'Charley's             918.  57.4 18.6  0.965  205.  1684.  65.3  34.4 27.0 \n 5 Romano's Macaroni Gri…  795.  48.8 17.7  0.457  162   1387.  56.2  34.7  8.33\n 6 Dickey's Barbeque Pit   788.  44.9 22.8  0.752  131.  2147.  52.7  40.5 16.0 \n 7 Yard House              685.  38.5 12.2  0.559  111.  1124.  50.5  29.8 17.8 \n 8 Outback Steakhouse      635.  39.2 16.8  0.941  132.  1142.  36.0  35.3 11.3 \n 9 Joe's Crab Shack        628.  33.8  9.06 0.0376 135.  1144.  51.3  33.0 16.9 \n10 PF Chang's              612.  27.1  6.17 0.132  102.  2033.  65.9  28.4 24.4 \n11 Carrabba's Italian Gr…  602.  30.5 11.6  0.330   88.8 1425.  52.2  27.1  9.32\n12 Zaxby's                 602.  27.6  5.17 0.364  104.  1545.  56.9  33.4 31.6"
  },
  {
    "objectID": "projects/SP4/Final_Phase1.html#numerical-summarymean-nutritional-facts-for-each-type-of-food",
    "href": "projects/SP4/Final_Phase1.html#numerical-summarymean-nutritional-facts-for-each-type-of-food",
    "title": "Final Project - Phase 1",
    "section": "Numerical Summary(Mean nutritional facts for each type of food)",
    "text": "Numerical Summary(Mean nutritional facts for each type of food)\n\n#We could make a healthy/non-healthy binary variable, but that's relative. \naverage_nutritionalFacts_ForFoodTypes &lt;- menu_stat %&gt;%  #might want to filter only the things that are not shareable, because the shareable items will probably make the averages go up because they are larger. \n  select(Year, restaurant, Item_Name, Restaurant_Item_Name, Food_Category, Calories, Total_Fat, Saturated_Fat, Trans_Fat, Cholesterol, Sodium, Potassium, Carbohydrates, Protein, Sugar, Limited_Time_Offer, Shareable) %&gt;% \n  filter(!is.na(Calories), !is.na(Total_Fat), !is.na(Saturated_Fat), !is.na(Trans_Fat), !is.na(Cholesterol), !is.na(Sodium), !is.na(Carbohydrates), !is.na(Protein), !is.na(Sugar)) %&gt;% \n  group_by(Food_Category) %&gt;% \n  summarise(cal = mean(Calories), Tfat = mean(Total_Fat), Sfat = mean(Saturated_Fat), TRfat = mean(Trans_Fat), Chol = mean(Cholesterol), Sod = mean(Sodium), Carb = mean(Carbohydrates), Prot = mean(Protein), Sug = mean(Sugar)) %&gt;% \n  arrange(desc(cal))\n\nhead(average_nutritionalFacts_ForFoodTypes, 12)\n\n# A tibble: 12 × 10\n   Food_Category            cal  Tfat  Sfat  TRfat  Chol   Sod  Carb  Prot   Sug\n   &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Burgers                 736. 42.8  15.4  1.27   115.  1414. 52.2  35.4  12.8 \n 2 Entrees                 672. 36.0  11.8  1.29   161.  1555. 52.2  36.0  11.8 \n 3 Sandwiches              595. 29.6  10.1  0.232  110.  1557. 51.8  30.6   7.40\n 4 Desserts                565. 27.7  15.0  0.460   77.1  317. 72.6   8.47 52.2 \n 5 Appetizers & Sides      528. 31.0   8.39 0.249   92.9 1310. 39.2  24.2   8.65\n 6 Fried Potatoes          491. 27.6   6.62 0.373   16.8  906. 51.7   8.12  2.49\n 7 Salads                  488. 31.0   7.98 0.159   90.7 1047. 28.3  26.0   9.60\n 8 Pizza                   371. 16.3   7.04 0.0976  41.6  867. 39.2  16.8   4.36\n 9 Baked Goods             316. 12.1   4.66 0.0832  15.2  449. 45.6   6.48 14.2 \n10 Soup                    303. 15.1   6.81 0.378   46.5 1300. 29.8  12.5   5.49\n11 Beverages               282.  6.03  3.75 0.0612  17.8  138. 51.5   4.17 46.1 \n12 Toppings & Ingredients  118.  6.87  2.05 0.121   16.7  298.  9.77  4.50  4.21"
  },
  {
    "objectID": "projects/SP4/Final_Phase1.html#numerical-summarynumber-of-fast-food-restaurants-in-each-state",
    "href": "projects/SP4/Final_Phase1.html#numerical-summarynumber-of-fast-food-restaurants-in-each-state",
    "title": "Final Project - Phase 1",
    "section": "Numerical Summary(Number of fast food restaurants in each state)",
    "text": "Numerical Summary(Number of fast food restaurants in each state)\n\nfast_food_locations %&gt;% \n  group_by(province) %&gt;% \n  summarise(`Number Of FF restaurants in that state` = n()) %&gt;% \n  arrange(desc(`Number Of FF restaurants in that state`))\n\n# A tibble: 50 × 2\n   province `Number Of FF restaurants in that state`\n   &lt;chr&gt;                                       &lt;int&gt;\n 1 CA                                           1201\n 2 TX                                            811\n 3 FL                                            621\n 4 OH                                            522\n 5 GA                                            420\n 6 IL                                            405\n 7 PA                                            383\n 8 MI                                            374\n 9 NY                                            352\n10 AZ                                            330\n# ℹ 40 more rows"
  },
  {
    "objectID": "projects/SP4/Final_Phase1.html#numerical-summarynumber-of-fast-food-restaurants-in-the-northern-united-states-versus-southern-united-states",
    "href": "projects/SP4/Final_Phase1.html#numerical-summarynumber-of-fast-food-restaurants-in-the-northern-united-states-versus-southern-united-states",
    "title": "Final Project - Phase 1",
    "section": "Numerical Summary(Number of fast food restaurants in the Northern United States versus Southern United States)",
    "text": "Numerical Summary(Number of fast food restaurants in the Northern United States versus Southern United States)\n\nfast_food_locations %&gt;% \n  mutate(North_OR_South = ifelse(latitude &gt; 37.5, \"North\", \"South\")) %&gt;% \n  group_by(North_OR_South) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(percentage = count/sum(count)*100)\n\n# A tibble: 2 × 3\n  North_OR_South count percentage\n  &lt;chr&gt;          &lt;int&gt;      &lt;dbl&gt;\n1 North           5112       51.1\n2 South           4888       48.9"
  },
  {
    "objectID": "projects/SP4/Final_Phase1.html#visualizationbarchart-with-the-most-popular-fastfood-restaurants",
    "href": "projects/SP4/Final_Phase1.html#visualizationbarchart-with-the-most-popular-fastfood-restaurants",
    "title": "Final Project - Phase 1",
    "section": "Visualization(Barchart with the most popular fastfood restaurants)",
    "text": "Visualization(Barchart with the most popular fastfood restaurants)\n\nfast_food_locations %&gt;%  \n  group_by(name) %&gt;% \n  summarize(count = n()) %&gt;% \n  arrange(desc(count)) %&gt;% \n  filter(count &gt;= 215) %&gt;% \n  ggplot() +\n  geom_col(aes(x = name, y = count), fill = \"darkblue\", color = \"lightblue\") +\n  theme_classic()+\n  theme(axis.text.x = element_text(angle=90, hjust=1))+\n  labs(x = \"Name of Restaurant\", y = \"Number of Locations in the Dataset\")"
  },
  {
    "objectID": "projects/SP4/Final_Phase1.html#visualizationaverage-amount-of-calories-in-a-fast-food-menu-item",
    "href": "projects/SP4/Final_Phase1.html#visualizationaverage-amount-of-calories-in-a-fast-food-menu-item",
    "title": "Final Project - Phase 1",
    "section": "Visualization(Average amount of calories in a fast food menu item)",
    "text": "Visualization(Average amount of calories in a fast food menu item)\n\naverage_nutritionalFacts_ForRestaurants %&gt;%  \n  ggplot() +\n  geom_density(aes(x = cal), fill = \"darkblue\", color = \"lightblue\", alpha = 0.7) +\n  theme_classic()+\n  labs(x = \"Calories\", y = \"Frequency\", title = \"Average Calories in Each Menu Item\")"
  },
  {
    "objectID": "projects/SP4/Final_Phase1.html#visualizationhow-does-type-of-food-effect-the-calories",
    "href": "projects/SP4/Final_Phase1.html#visualizationhow-does-type-of-food-effect-the-calories",
    "title": "Final Project - Phase 1",
    "section": "Visualization(How does type of food effect the calories?)",
    "text": "Visualization(How does type of food effect the calories?)\n\naverage_nutritionalFacts_ForFoodTypes %&gt;% \nggplot() +\n  geom_col(aes(x = Food_Category, y = cal), fill = \"darkblue\", color = \"lightblue\") +\n  theme_classic()+\n  theme(axis.text.x = element_text(angle=90, hjust=1))+\n  labs(x = \"Name of Restaurant\", y = \"Number of Locations in the Dataset\")"
  },
  {
    "objectID": "projects/SP3/Project_Code_and_Data.html",
    "href": "projects/SP3/Project_Code_and_Data.html",
    "title": "Project ideas",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rvest)\nlibrary(stringr)\nmajordata &lt;- read_csv(\"majordata.csv\")"
  },
  {
    "objectID": "projects/SP3/Project_Code_and_Data.html#including-plots",
    "href": "projects/SP3/Project_Code_and_Data.html#including-plots",
    "title": "Project ideas",
    "section": "Including Plots",
    "text": "Including Plots\n\nResearch question idea: How do different majors vary in the average number of classes taken in each division? How does\n\n\n# ----CODE FOR TAB NUMBER 3 IN THE SHINY APP----\n\n# here we are mainly just pivoting the major data into a longer format so that it's easier to work with.\nmajors &lt;- majordata%&gt;%\n  pivot_longer(cols = c(`major1`, `major2`, `major3`), names_to = \"major#\", values_to = \"major\")%&gt;%\n  pivot_longer(cols = c(`major1_division`, `major2_division`, `major3_division`), names_to = \"major#_division\", values_to = \"major_division\") %&gt;%\n  filter(!is.na(major), !is.na(major_division)) %&gt;%\n  filter(`major#` == str_sub(`major#_division`,1,6))%&gt;%\n  select(\"StudentID\", \"major#\", \"major\", \"major#_division\", \"major_division\", \"FineArts_count\", \"Humanities_count\", \"Interdisciplinary_count\", \"NatScienceandMath_count\", \"SocialSciences_count\", \"NonDivisional_count\", \"admit_type\")%&gt;%\n  rename(`Fine Arts` = FineArts_count)%&gt;%\n  rename(`Natural Sciences and Mathematics` = NatScienceandMath_count)%&gt;%\n  rename(`Humanities` = Humanities_count)%&gt;%\n  rename(`Social Sciences` = SocialSciences_count)%&gt;%\n  rename(`Interdisciplinary` = Interdisciplinary_count)%&gt;%\n  rename(`NonDivisional` = NonDivisional_count)\n  \n# The number of classes that each student took in the 5 divisions(in long format)\nmajors2 &lt;- majors%&gt;%\n  pivot_longer(cols = c(`Fine Arts`, `Humanities`, `Interdisciplinary`, `Natural Sciences and Mathematics`, `Social Sciences`, `NonDivisional`), names_to = \"division\", values_to = \"numOfClasses\")\n\n# This is the average number of classes that a student in each major would take across all of the divisions. \nDivisions_By_Major &lt;- majors%&gt;%\n  group_by(major)%&gt;%\n  summarise(countFA = mean(`Fine Arts`), countHUM = mean(Humanities), countINT = mean(Interdisciplinary), countSCIMATH = mean(`Natural Sciences and Mathematics`), countSOCSCI = mean(`Social Sciences`), countNON = mean(NonDivisional))\n\n# The average number of classes the a student in each major would take outside of their majors division.\nOutsideMajor &lt;- majors2%&gt;%\n  filter(!(major_division == division))%&gt;%\n  group_by(StudentID, major)%&gt;%\n  summarise(num = sum(numOfClasses))\n\n# average number of classes taken outside of your majors division(one number summary). The average number of classes taken outside your majors division is 24.67.\nOneNumberSummary &lt;- majors2%&gt;%\n  filter(!(admit_type == \"TRN\"))%&gt;%\n  filter(!(major_division == division))%&gt;%\n  group_by(StudentID)%&gt;%\n  summarise(num = sum(numOfClasses))%&gt;%\n  summarise(Average = sum(num)/n())\n\n# average number of classes taken in each division by major, along with the division that each major falls in. \nmajorsViz &lt;- OutsideMajor%&gt;%   \n  group_by(major)%&gt;%\n  summarize(avg = mean(num))%&gt;%\n  arrange(desc(avg))\n\nmajorsViz &lt;- majorsViz%&gt;% #join with division_by_major\n  left_join(majors)%&gt;%\n  left_join(Divisions_By_Major)%&gt;%\n  distinct(major, .keep_all = TRUE)%&gt;%\n  select(major, avg, major_division, countFA, countHUM, countINT, countSCIMATH, countSOCSCI, countNON)\n\n# This is the same data as \"majorsViz\" above but in the longer format.**This is the data used for the third viz on the shiny app.**\nmajorsViz2 &lt;- majorsViz%&gt;%\n  group_by(major_division)%&gt;%\n  pivot_longer(cols = c(countFA, countHUM, countINT, countSCIMATH, countSOCSCI, countNON), names_to = \"division\", values_to = \"avgClasses\")\n\nmajorsViz2%&gt;%\n  group_by(major_division)%&gt;%\n  summarise(classesOutside = mean(avg))\n\n#----CODE FOR TAB NUMBER 1 IN THE SHINY APP----\n# average number of classes taken in each division based on the number of majors a student has. \nmajo &lt;- majordata %&gt;%\n  group_by(major_ct) %&gt;%\n  summarise(fine_arts = mean(FineArts_count),\n            humanities = mean(Humanities_count),\n            natscience = mean(NatScienceandMath_count),\n            social_science = mean(SocialSciences_count),\n            nonDivis = mean(NonDivisional_count),\n            inter= mean(Interdisciplinary_count))%&gt;%\n  rename(`Fine Arts` = fine_arts)%&gt;%\n  rename(`Humanities` = humanities)%&gt;%\n  rename(`Social Sciences` =  social_science)%&gt;%\n  rename(`Natural Science and Mathematics` = natscience)%&gt;%\n  rename(`Non Divisional` = nonDivis)%&gt;%\n  rename(`Interdisciplinary` = inter)\n# long version of the above.  **Data used for the first viz on the shiny app.**\nmajjors &lt;- majo %&gt;%\n  pivot_longer(cols = !\"major_ct\", names_to = \"division\")\n\n#----CODE FOR TAB NUMBER 2 IN THE SHINY APP----\n# this is the average number of prefixes for Macalester students(one number summary)\naveragePrefix&lt;-majordata %&gt;% \n  summarize(\"Average Amount of Prefixes of All Students\"=mean(prefix_count,na.rm=TRUE),\"Medain Prefix\"=median(prefix_count))\naveragePrefix\n\n#Filtering out all of the students that have more then one major.\nsingleMajors&lt;-majordata %&gt;% \n  filter(is.na(major2))\n\n\nsingleMajors%&gt;% \n  mutate(across(where(is.character), as.factor)) %&gt;% \n  summary()\n\n#average number of prefixes taken for each major at Macalester.\navgPrefix&lt;-singleMajors %&gt;% \n  group_by(major1) %&gt;% \n  summarize(avgPrefix=mean(prefix_count,na.rm=TRUE)) %&gt;% \n  arrange(desc(avgPrefix)) \n\n#Same as \"avgPrefix\", but added a variable for the division that each major falls in. **Data used for the second viz on the shiny app.**\navgPrefix2 &lt;- avgPrefix %&gt;% \n  left_join(singleMajors)%&gt;%\n  select(major1, avgPrefix, major1_division)%&gt;%\n  distinct()\n\n\n# ------ PRACTICE VISUALIZATIONS------\n\n#Visualization No. 1\ntest &lt;- majorsViz2%&gt;%\n  ggplot(aes(x = major, y= avgClasses, fill = division))+\n  geom_col(position = \"fill\")+\n  theme(axis.text.x = element_text(angle = 90))\n\n#Visualization No. 2***\nviz1 &lt;- majorsViz2%&gt;%\n  ggplot(aes(x = major, y = avgClasses))+\n  geom_point(aes(shape = division, color = major_division))+\n  theme(axis.text.x = element_text(angle = 90))+\n  labs(title = \"Distrubution of Classes Based on Your Major\", y = \"Average Number of Classes\", x = \"Major\")\n\n#Visualization No. 3\nmajorsViz2%&gt;%\n  ggplot(aes(x = major, y = avgClasses))+\n  geom_point(aes(shape = division, color = major_division))+\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nggplotly(test)\n\n\n\n\nggplotly(viz1)\n\n\n\n\n\n\n#This is the custom theme that each one of the visualizations in the shiny app are using. \n\ntheme_gppr &lt;- function(){ \n    font &lt;- \"Georgia\"   #assign font family up front\n    \n    theme_minimal() %+replace%    #replace elements we want to change\n    \n    theme(\n      \n      #grid elements\n      panel.grid.major = element_blank(),    #strip major gridlines\n      panel.grid.minor = element_blank(),    #strip minor gridlines\n      axis.ticks = element_blank(),          #strip axis ticks\n      \n      #since theme_minimal() already strips axis lines, \n      #we don't need to do that again\n      \n      #text elements\n      plot.title = element_text(             #title\n                   family = font,            #set font family\n                   size = 15,                #set font size\n                   face = 'bold',            #bold typeface\n                   hjust = 0,                #left align\n                   vjust = 2),               #raise slightly\n      \n      plot.subtitle = element_text(          #subtitle\n                   family = font,            #font family\n                   size = 11),               #font size\n      \n      plot.caption = element_text(           #caption\n                   family = font,            #font family\n                   size = 9,                 #font size\n                   hjust = 1),               #right align\n      \n      axis.title = element_text(             #axis titles\n                   family = font,            #font family\n                   size = 10),               #font size\n      \n      axis.text = element_text(              #axis text\n                   family = font,            #axis famuly\n                   size = 9,angle=90),                #font size\n      \n      axis.text.x = element_text(            #margin for axis text\n                    margin=margin(5, b = 10))\n      \n      #since the legend often requires manual tweaking \n      #based on plot content, don't define it here\n    )\n}"
  },
  {
    "objectID": "projects/Projects.html",
    "href": "projects/Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Modeling Trade Shares of GDP vs Natural Disasters\n\nHow does the number of natural disasters effect the trade share of GDP?\nView disasters VS trade project\n\n\n\nExploring Premier League SPI Ratings\n\nWhat types of things impact the Soccer Power Index of a Premier League club?\nExplore Premier League Rankings\n\n\n\nLiberal Arts Education at Macalester College\n\nWhat does a liberal arts education look like at Macalester College? What sorts of classes do students take? What kind of departments do they participate in?\nExplore an Education at Macalester College\n\n\n\nComparing Fast Food’s\n\nAre there better/healthier caloric fast food restaurants?\nExplore Fast Foods Restaurants Code Explore Part 1 of analysis Explore Part 2 of analysis\n\n\n\nGenome Wide Association Study\n\nSTAT 494 - Content Summary #1\nExplore a GWAS Simulation + Summary\nExplore a GWAS Simulation + Summary(Outcome depends on SNP)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome to my personal website, created using Quarto in R/RStudio. My name is Nick Kent. I am a junior at Macalester College pursuing a major in data science, and minors in computer science and economics. I have been gaining many skills in the realms of data science, computer science, economics. I enjoy data exploration and analysis in a variety of field and topics, and hope to continue to grow my skillset. Please take a look through this website if you would like to learn more about myself and some of my work.\n\n\n\n\n\n\n\n\n\nI spend lots of time playing soccer. During the fall(and the majority of the school year) I compete as a member of the men’s soccer team at Macalester College, and over the summer I spend time playing with Minneapolis City Soccer Club.\n \nI also enjoy spending time with friends and family outdoors, and trying new restaurants. My favorite places to hangout in Minneapolis are the lakes in Minneapolis(Harriet, Bde Maka Ska, Cedar, Isles). Furthermore, I enjoy restaurants such as Black Sheep Pizza, World Street Kitchen, Quangs, and Andale Taqueria."
  }
]